{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib\n",
    "\n",
    "źródło: https://spark.apache.org/docs/3.0.1/ml-guide.html\n",
    "\n",
    "MLlib to biblioteka przeznaczona do uczenia maszynowego w Sparku. Jej celem jest prostego, skalowalnego rozwiązania w zakresie ML. Biblioteka zapewnia:\n",
    "\n",
    "1. Algorytmy ML: klasyfikacja, regresja, klastrowanie i collaborative filtering\n",
    "2. Featuryzacja: ekstrakcja zmiennych, transformacja, redukcja wymiarowości i selekcja\n",
    "3. Pipelines: narzędzia do konstruowania, oceniania i dostrajania pipeline'ów ML\n",
    "4. Trwałość: metody zapisywania i wczytywania algorytmów, modeli i pipeline'ów\n",
    "5. Pozostałe narzędzia: algebra liniowa, statystyka, obsługa danych itp.\n",
    "\n",
    "### Statystyki podstawowe\n",
    "\n",
    "**Korelacja** </br>\n",
    "\n",
    "Obliczanie korelacji między zmiennymi jest must - have. Na dzień dzisiejszy obsługiwane metody to korelacja Pearsona i Spearmana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Może chwilę potrwać\n",
    "spark = SparkSession.builder.appName(\"UczymySięSparka\").getOrCreate()\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testowanie hipotez**\n",
    "Testowanie hipotez jest bardzo ważne podczas określenia, czy wynik jest statystycznie istotny. Spark.ml obecnie obsługuje chi-kwadrat Pearsona (χ2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.6872892787909721,0.6822703303362126]\n",
      "degreesOfFreedom: [2, 3]\n",
      "statistics: [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podsumowanie**\n",
    "Spark udostępnia statystyki podsumowujące za pośrednictwem narzędzia Summarizer. Dostępne metryki to maks., min, średnia, suma, wariancja, std i liczba niezerowych w kolumnach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|aggregate_metrics(features, weight)|\n",
      "+-----------------------------------+\n",
      "|{[1.0,1.0,1.0], 1}                 |\n",
      "+-----------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|aggregate_metrics(features, 1.0)|\n",
      "+--------------------------------+\n",
      "|{[1.0,1.5,2.0], 2}              |\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.0,1.0] |\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.5,2.0] |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.sparkContext.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "                     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    "\n",
    "# create summarizer for multiple metrics \"mean\" and \"count\"\n",
    "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
    "\n",
    "# compute statistics for multiple metrics with weight\n",
    "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# compute statistics for multiple metrics without weight\n",
    "df.select(summarizer.summary(df.features)).show(truncate=False)\n",
    "\n",
    "# compute statistics for single metric \"mean\" with weight\n",
    "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# compute statistics for single metric \"mean\" without weight\n",
    "df.select(Summarizer.mean(df.features)).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Pipelines ###\n",
    "W tej sekcji przedstawiamy koncepcję pipeline'ów ML. Potoki ML zapewniają jednolity zestaw API zbudowanych w oparciu o ramki, które pomagają użytkownikom tworzyć i dostrajać pipeline'y.\n",
    "\n",
    "**Główne pojęcia w Pipelines**\n",
    "MLlib wystandaryzowało ML API żeby ułatwić łączenie wielu algorytmów w jeden pipeline lub worflow. W zaczynającej się teraz części warsztatu omówimy kluczowe koncepcje API Pipelines. Warto dodać, że API projektowane było na modłę projektu scikit-learn.\n",
    "\n",
    "* **DataFrame**: to API wykorzystuje ramkę ze Spark SQL w charakterze danych pod modelowanie. Taka ramka może przechowywać różne typy danych np. tekst, wektory cech, empiryczne etykiety i prognozowane etykiety.\n",
    "\n",
    "* **Transformator**: Transformator to algorytm, który może przekształcić jedną ramkę danych w inną ramkę danych. Np. model ML to Transformer, który przekształca DataFrame empiryczne w DataFrame z danymi prognostycznymi.\n",
    "\n",
    "* **Estimator**: Estimator to algorytm, który można dopasować (fit) do ramki w celu stworzenia transformatora. Np. algorytm uczący się to estymator, który uczy się na ramce i tworzy model.\n",
    "\n",
    "* **Pipeline**: łączy ze sobą wiele transformatorów i estymatorów, aby określić ML workflow.\n",
    "\n",
    "* **Parametr**: Wszystkie transformatory i estymatory mają teraz wspólne API do tuningowania parametrów.\n",
    "\n",
    "### Elementy pipeline'a ###\n",
    "**Transformatory** </br>\n",
    "\n",
    "Transformator to obiekt, który obejmuje zarówno transformatory featuresów jak i modele. Z technicznego punktu widzenia Transformer implementuje metodę transform(), która konwertuje jedną ramkę na inną, zazwyczaj poprzez dołączenie jednej lub więcej kolumn. Na przykład:\n",
    "\n",
    "Transformator featuresa może pobrać ramkę, odczytać kolumnę (np. tekst), zmapować ją na nową kolumnę i zwrócić nową ramkę z dołączoną zmapowaną kolumną.\n",
    "Model może pobrać ramkę, odczytać wektory cech, wykonać predykcję etykiety i zwrócić nową ramkę z prognozowanymi etykietami jako nową kolumnę ramki.\n",
    "\n",
    "### Estymatory ###\n",
    "\n",
    "Estymator to wynik algorytmu uczącego się. Z technicznego punktu widzenia Estimator implementuje metodę fit(), która przyjmuje jako argument ramkę i tworzy model, który jest Transformerem. Na przykład algorytm uczący się, taki jak LogisticRegression, jest estymatorem, a wywołanie funkcji fit() trenuje ten model, który dzięki temu staje się transformatorem.\n",
    "\n",
    "### Pipeline ###\n",
    "W ML powszechne jest uruchamianie sekwencji algorytmów w celu przetwarzania danych i uczenia modeli. Dla przykładu -  proces przetwarzania dokumentu tekstowego może obejmować kilka etapów:\n",
    "\n",
    "* Podziel tekst każdego dokumentu na słowa.\n",
    "* Konwertuj słowa każdego dokumentu na wektor numeryczny.\n",
    "* Wyuczenie modelu przy użyciu wektorów cech i etykiet.\n",
    "\n",
    "MLlib reprezentuje taki przepływ pracy jako Pipeline, który składa się z sekwencji PipelineStages (Transformers i Estimators) uruchamianych w określonej kolejności. W tej sekcji, jako przykladu, użyjemy prostego workflow.\n",
    "\n",
    "### Działanie pipeline'a ###\n",
    "Pipeline jest sekwencją etapów, a każdy etap jest albo Transformatorem, albo Estymatorem. Elementy uruchamiane są w określonej kolejności, a wejściowa ramka jest przekształcana podczas przechodzenia przez każdy z etapów. W przypadku etapu Transformer na ramce wywoływana jest metoda transform(). W przypadku etapu Estimator w celu stworzenia Transformera wywoływana jest metoda fit() (Transformer staje się wtedy częścią pipeline'u modelu lub estymatora), a metoda transform() jest wykonywana na ramce.\n",
    "\n",
    "Poniżej prosta ilustracja:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyżej górny rząd przedstawia pipeline z trzema etapami. Pierwsze dwa (Tokenizer i HashingTF) to Transformers (niebieski), a trzeci (LogisticRegression) to Estimator (czerwony). Dolny wiersz reprezentuje dane przepływające przez pipeline, gdzie cylindry oznaczają ramki danych. Metoda Pipeline.fit() jest wywoływana na oryginalnej ramce, która zawiera nieprzetworzone dokumenty tekstowe i etykiety. Metoda Tokenizer.transform() dzieli nieprzetworzone dokumenty tekstowe na słowa, dodając nową kolumnę do ramki. Metoda HashingTF.transform() konwertuje kolumnę tekstową na wektor numeryczny, dodając nową kolumnę, zakodowaną kolumnę do ramki. Teraz, ponieważ LogisticRegression jest Estimatorem, Pipeline najpierw wywołuje LogisticRegression.fit() w celu utworzenia LogisticRegressionModel. Gdyby pipline miał więcej estymatorów, wywołałby metodę transform() LogisticRegressionModel na ramce przed przekazaniem jej do następnego etapu.\n",
    "\n",
    "Pipeline jest estymatorem. Zatem po uruchomieniu metody fit() Pipeline tworzy PipelineModel, który jest Transformerem. Ten PipelineModel jest używany w czasie testu; poniższy rysunek to ilustruje:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na powyższym rysunku PipelineModel ma taką samą liczbę etapów jak oryginalny Pipeline, ale wszystkie estymatory w oryginalnym Pipeline stały się Transformatorami. Kiedy metoda transform() obiektu PipelineModel jest wywoływana na testowym zbiorze danych, dane są przekazywane przez dopasowany (fitted) pipeline w określonej kolejności. Metoda transform() każdego etapu aktualizuje zestaw danych i przekazuje go do następnego etapu.\n",
    "\n",
    "Pipelines i PipelineModels pomagają zagwarantować, że dane treningowe i testowe przechodzą przez identyczne etapy przetwarzania.\n",
    "\n",
    "### Parametry ###\n",
    "\n",
    "Estymatory i transformatory MLlib używają jednolitego API do określania parametrów.\n",
    "\n",
    "Param to konkretny parametr z odpowiednią dokumentacją. ParamMap to zestaw par typu klucz - wartość.\n",
    "\n",
    "Istnieją dwa główne sposoby przekazywania parametrów do algorytmu:\n",
    "\n",
    "1. Ustaw parametry dla instancji. Przykładowo, jeśli lr jest instancją LogisticRegression, można wywołać lr.setMaxIter(10), aby lr.fit() użyła maksymalnie 10 iteracji. \n",
    "2. Przekaż ParamMap do fit() lub transform(). Wszelkie parametry w ParamMap zastąpią parametry określone wcześniej za pomocą setter methods.\n",
    "\n",
    "Parametry należą do konkretnych instancji Estymatorów i Transformatorów. Na przykład, jeśli mamy dwie instancje LogisticRegression lr1 i lr2, możemy zbudować ParamMap z podanymi dwoma parametrami maxIter: ParamMap(lr1.maxIter -> 10, lr2.maxIter -> 20). Jest to przydatne, jeśli istnieją dwa algorytmy z parametrem maxIter w pipeline.\n",
    "\n",
    "### Zapisywanie i ładowanie pipeline'ów ###\n",
    "Często warto zapisać model lub pipeline na dysku. Od wersji Spark 2.3 API biblioteki spark.ml i pyspark.ml pokrywają oba te aspekty.\n",
    "\n",
    "Zapisywanie modeli i pipeline'ów działa w Scali, Javie i Pythonie. R aktualnie używa zmodyfikowanego formatu, więc modele zapisane w R można wczytać z powrotem tylko w R.\n",
    "\n",
    "### Poniżej przykłady: ### \n",
    "**Estymator, transformer i param**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Prepare training data from a list of (label, features) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training)\n",
    "\n",
    "# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n",
    "# we can view the parameters it used during fit().\n",
    "# This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "# LogisticRegression instance.\n",
    "print(\"Model 1 was fit using parameters: \")\n",
    "print(model1.extractParamMap())\n",
    "\n",
    "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Specify multiple Params.\n",
    "\n",
    "# You can combine paramMaps, which are python dictionaries.\n",
    "paramMap2 = {lr.probabilityCol: \"myProbability\"}  # Change output column name\n",
    "paramMapCombined = paramMap.copy()\n",
    "paramMapCombined.update(paramMap2)\n",
    "\n",
    "# Now learn a new model using the paramMapCombined parameters.\n",
    "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "model2 = lr.fit(training, paramMapCombined)\n",
    "print(\"Model 2 was fit using parameters: \")\n",
    "print(model2.extractParamMap())\n",
    "\n",
    "# Prepare test data\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# LogisticRegression.transform will only use the 'features' column.\n",
    "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
    "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "prediction = model2.transform(test)\n",
    "result = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "for row in result:\n",
    "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
    "          % (row.features, row.label, row.myProbability, row.prediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Przykład pipeline'u:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.6292098489668488,0.37079015103315116], prediction=0.000000\n",
      "(5, l m n) --> prob=[0.984770006762304,0.015229993237696027], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.13412348342566147,0.8658765165743385], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9955732114398529,0.00442678856014711], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ekstrakcja, przekształcanie i wybieranie cech ###\n",
    "\n",
    "Ta sekcja obejmuje popularne transformacje wektorów zmiennych. \n",
    "\n",
    "**Binaryzacja** </br>\n",
    "\n",
    "Binaryzacja to proces mapowania cech liczbowych do cech binarnych (0/1). Binarizer przyjmuje wspólne parametry inputCol i outputCol, a także próg binaryzacji. Wartości funkcji większe niż próg są binaryzowane do 1; wartości równe lub mniejsze od progu są binaryzowane do 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.500000\n",
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    0.1|              0.0|\n",
      "|  1|    0.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 0.1),\n",
    "    (1, 0.8),\n",
    "    (2, 0.2)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "\n",
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "binarizedDataFrame.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA** </br>\n",
    "\n",
    "PCA to procedura statystyczna wykorzystująca transformację ortogonalną do przetransformowania zbioru potencjalnie skorelowanych zmiennych na zbiory nieskorelowanych cech statystycznych. Powstałe zbiory nazywane są głównymi składowymi. Poniższy przykład pokazuje, jak rzutować 5-wymiarowe wektory na 3-wymiarowe główne składowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|pcaFeatures                                                 |\n",
      "+------------------------------------------------------------+\n",
      "|[1.6485728230883814,-4.0132827005162985,-1.0091435193998504]|\n",
      "|[-4.645104331781533,-1.1167972663619048,-1.0091435193998501]|\n",
      "|[-6.428880535676488,-5.337951427775359,-1.009143519399851]  |\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StringIndexer** </br>\n",
    "\n",
    "StringIndexer mapuje kolumnę etykiet na kolumnę indeksów etykiet. StringIndexer może kodować wiele kolumn. Indeksy zawierają się w przedziale [0, numLabels) i obsługiwane są cztery opcje szeregowania ich: \n",
    "* „frequencyDesc”: kolejność malejąca według częstotliwości etykiety (najczęstsza etykieta ma przypisywane 0),\n",
    "* „frequencyAsc”: kolejność rosnąca według częstotliwości etykiety (najrzadziej występujaca etykieta ma przypisane 0), \n",
    "* „alphabetDesc”: malejąca kolejność alfabetyczna\n",
    "* „alphabetAsc”: rosnąca kolejność alfabetyczna (domyślnie = „frequencyDesc”). \n",
    "\n",
    "Warto zapamiętać, że w przypadku tej samej częstotliwości w przypadku użycia „frequencyDesc”/”frequencyAsc”, ciągi są dalej sortowane według alfabetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IndexToString** </br>\n",
    "\n",
    "IndexToString mapuje kolumnę indeksów etykiet z powrotem do kolumny zawierającej oryginalne etykiety. Typowym przypadkiem użycia jest tworzenie indeksów z etykiet za pomocą StringIndexer, trenowanie modelu na tych indeksach i pobieranie oryginalnych etykiet za pomocą IndexToString. Można też wykorzystać własne etykiety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column 'category' to indexed column 'categoryIndex'\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "StringIndexer will store labels in output column metadata\n",
      "\n",
      "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
      "+---+-------------+----------------+\n",
      "| id|categoryIndex|originalCategory|\n",
      "+---+-------------+----------------+\n",
      "|  0|          0.0|               a|\n",
      "|  1|          2.0|               b|\n",
      "|  2|          1.0|               c|\n",
      "|  3|          0.0|               a|\n",
      "|  4|          0.0|               a|\n",
      "|  5|          1.0|               c|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
    "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.show()\n",
    "\n",
    "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
    "\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHotEncoder** </br>\n",
    "\n",
    "Mając kategorie zamienione na odpowiadające im liczby możemy zamienić je także na kilka kolumn (ich liczba zależy od tego ile jest kategorii), które zawierają zera i jedynki oznaczające odpowiednio czy dany wiersz należy do kategorii czy nie. Metodę tę stosujemy, gdy używamy algorytmu, który może mieć problem ze zmiennymi liczbowymi (bo zakładają jakiś porządek)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Efekt interakcji** </br>\n",
    "\n",
    "Efekt interakcji to Transformer, który generuje pojedynczą kolumnę wektorową zawierającą iloczyn wszystkich kombinacji wartości z kolumn wejściowych.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------+------------------------------------------------------+\n",
      "|id1|vec1          |vec2          |interactedCol                                         |\n",
      "+---+--------------+--------------+------------------------------------------------------+\n",
      "|1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] |[8.0,4.0,5.0,16.0,8.0,10.0,24.0,12.0,15.0]            |\n",
      "|2  |[4.0,3.0,8.0] |[7.0,9.0,8.0] |[56.0,72.0,64.0,42.0,54.0,48.0,112.0,144.0,128.0]     |\n",
      "|3  |[6.0,1.0,9.0] |[2.0,3.0,6.0] |[36.0,54.0,108.0,6.0,9.0,18.0,54.0,81.0,162.0]        |\n",
      "|4  |[10.0,8.0,6.0]|[9.0,4.0,5.0] |[360.0,160.0,200.0,288.0,128.0,160.0,216.0,96.0,120.0]|\n",
      "|5  |[9.0,2.0,7.0] |[10.0,7.0,3.0]|[450.0,315.0,135.0,100.0,70.0,30.0,350.0,245.0,105.0] |\n",
      "|6  |[1.0,1.0,4.0] |[2.0,8.0,4.0] |[12.0,48.0,24.0,12.0,48.0,24.0,48.0,192.0,96.0]       |\n",
      "+---+--------------+--------------+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Interaction, VectorAssembler\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1, 2, 3, 8, 4, 5),\n",
    "     (2, 4, 3, 8, 7, 9, 8),\n",
    "     (3, 6, 1, 9, 2, 3, 6),\n",
    "     (4, 10, 8, 6, 9, 4, 5),\n",
    "     (5, 9, 2, 7, 10, 7, 3),\n",
    "     (6, 1, 1, 4, 2, 8, 4)],\n",
    "    [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\", \"id6\", \"id7\"])\n",
    "\n",
    "assembler1 = VectorAssembler(inputCols=[\"id2\", \"id3\", \"id4\"], outputCol=\"vec1\")\n",
    "\n",
    "assembled1 = assembler1.transform(df)\n",
    "\n",
    "assembler2 = VectorAssembler(inputCols=[\"id5\", \"id6\", \"id7\"], outputCol=\"vec2\")\n",
    "\n",
    "assembled2 = assembler2.transform(assembled1).select(\"id1\", \"vec1\", \"vec2\")\n",
    "\n",
    "interaction = Interaction(inputCols=[\"id1\", \"vec1\", \"vec2\"], outputCol=\"interactedCol\")\n",
    "\n",
    "interacted = interaction.transform(assembled2)\n",
    "\n",
    "interacted.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StandardScaler** </br>\n",
    "\n",
    "StandardScaler to Transformer, który przekształca zestaw wektorów tak, żeby miał średnią = 0 i odchylenie standardowe = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "dataFrame = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucketizer** </br>\n",
    "\n",
    "Bucketizer przekształca kolumnę ciągłą w kolumnę kategoryczną. Wartości graniczne dla danej kategorii ustala użytkownik. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 4 buckets\n",
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|  -999.9|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.2|             2.0|\n",
      "|   999.9|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "\n",
    "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
    "dataFrame = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChiSqSelector**\n",
    "\n",
    "ChiSqSelector oznacza wybór zmiennych na podstawie testu Chi-kwadrat. ChiSqSelector wykorzystuje test niezależności Chi-kwadrat, aby zdecydować, które zmienne wybrać. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 1 features selected\n",
      "+---+------------------+-------+----------------+\n",
      "| id|          features|clicked|selectedFeatures|\n",
      "+---+------------------+-------+----------------+\n",
      "|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n",
      "|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n",
      "|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n",
      "+---+------------------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "\n",
    "result = selector.fit(df).transform(df)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RFormula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[0.0,0.0,18.0]|  1.0|\n",
      "|[1.0,0.0,12.0]|  0.0|\n",
      "|[0.0,1.0,15.0]|  0.0|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "dataset = spark.createDataFrame(\n",
    "    [(7, \"US\", 18, 1.0),\n",
    "     (8, \"CA\", 12, 0.0),\n",
    "     (9, \"NZ\", 15, 0.0)],\n",
    "    [\"id\", \"country\", \"hour\", \"clicked\"])\n",
    "\n",
    "formula = RFormula(\n",
    "    formula=\"clicked ~ country + hour\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "\n",
    "output = formula.fit(dataset).transform(dataset)\n",
    "output.select(\"features\", \"label\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelowanie ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regresja logistyczna** </br>\n",
    "\n",
    "Używana, gdy zmienna zależna jest na skali dychotomicznej. Wartości zmiennej objaśnianej wskazują na wystąpienie, lub brak wystąpienia pewnego zdarzenia, które chcemy prognozować. Regresja logistyczna pozwala na obliczanie prawdopodobieństwa tego zdarzenia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient boosting** \n",
    "\n",
    "Gradient Boost jest algorytmem z rodziny metod zespołowych, a konkretnie boostingu. Metoda ta wykorzystuje wiele prostych algorytmów typu drzewo decyzyjne, aby zredukować zarówno bias i wariancje modelu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputy/Outputy modelu: https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#gradient-boosted-trees-gbts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron wielowarstwowy (ang. Multilayer Perceptron, MLP)**\n",
    "\n",
    "Najpopularniejszy typ sieci neuronowych. Sieć tego typu składa się zwykle z jednej warstwy wejściowej, kilku warstw ukrytych oraz jednej warstwy wyjściowej. Ustalenie właściwej liczby warstw ukrytych oraz liczby neuronów znajdujących się w poszczególnych warstwach jest zadaniem dla zespołu budującego sieć. Warstwa wyjściowa może składać się z neuronów liniowych (w przypadku regresji) lub neuronów nieliniowych (w przypadku klasyfikacji). Trenowanuje się ją przy użyciu propagacji wstecznej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"data/mllib/sample_multiclass_classification_data.txt\")\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [4, 5, 4, 3]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# train the model\n",
    "model = trainer.fit(train)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naiwny klasyfikator bayesowski**\n",
    "\n",
    "Prosty klasyfikator probabilistyczny. Naiwne klasyfikatory bayesowskie są oparte na założeniu o wzajemnej niezależności predyktorów. Często nie mają one żadnego związku z rzeczywistością i właśnie z tego powodu nazywa się je naiwnymi. Bardziej opisowe jest określenie – „model cech niezależnych”. Ponadto model prawdopodobieństwa można wyprowadzić korzystając z twierdzenia Bayesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\") \\\n",
    "    .load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalized linear regression**\n",
    "\n",
    "Uogólniony model liniowy rozszerza ogólny model liniowy w taki sposób, że zmienna zależna jest liniowo powiązana z wektorami cech za pośrednictwem określonej funkcji wiążącej. Model przyjmuje zmienną zależną, które nie ma rozkładu normalnego. Dzięki bardzo ogólnej postaci modelu obejmuje on wiele modeli statystycznych, takich jak regresja liniowa dla odpowiedzi o rozkładzie normalnym, modele logistyczne dla danych binarnych, modele logarytmiczno-liniowe dla danych o liczebności i wiele innych modeli statystycznych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "# Load training data\n",
    "dataset = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(dataset)\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Tuning: tuning hiperparametrów i selekcja modelu ###\n",
    "\n",
    "W tej sekcji opisano jak używać narzędzi MLlib do tuningowania algorytmów i pipeline'ów ML. Umożliwiają to dostępne w bibliotece funkcje Cross-Validation i inne.\n",
    "\n",
    "**Wybór modelu (inaczej dostrajanie hiperparametrów)**\n",
    "\n",
    "Ważnym zadaniem w ML jest wybór modelu. Nazywa się to również dostrajaniem. Tuning można przeprowadzić dla poszczególnych estymatorów, takich jak LogisticRegression, lub dla całych pipeline'ów, które obejmują wiele algorytmów, featuryzację i inne. Użytkownicy mogą dostroić cały pipeline jednocześnie zamiast dostrajać każdy element osobno.\n",
    "\n",
    "MLlib pozwala zdecydować o wyborze modelu za pomocą narzędzi, takich jak CrossValidator i TrainValidationSplit. Wymagają one następujących elementów:\n",
    "\n",
    "* Estimator: algorytm lub Pipeline do dostrojenia\n",
    "* Zestaw ParamMaps: parametry do wyboru, czasami nazywane „siatką parametrów” do przeszukania\n",
    "* Ewaluator: miara do zbadania, jak dobrze dopasowany Model radzi sobie z danymi testowymi\n",
    "\n",
    "Ewaluatorem może być RegressionEvaluator dla zagadnień regresyjnych, BinaryClassificationEvaluator dla modeli binarnych, MulticlassClassificationEvaluator dla modeli wieloklasowych lub MultilabelClassificationEvaluator dla klasyfikacji z wieloma etykietami. Domyślna metryka używana do wyboru najlepszej pary ParamMap może zostać zastąpiona przez metodę setMetricName w każdym z tych ewaluatorów.\n",
    "\n",
    "ParamGridBuilder może pomóc w konstrukcji siatki parametrów. Domyślnie zestawy parametrów z siatki są oceniane szeregowo. Ocenę parametrów można przeprowadzić równolegle, ustawiając partycjonowanie na wartość 2 lub większą (wartość 1 wykona obliczenia szeregowo) przed rozpoczęciem wyboru modelu za pomocą narzędzia CrossValidator lub TrainValidationSplit. Wartość dla paremetru określającego partycjonowanie należy wybierać ostrożnie, aby zmaksymalizować możliwości bez przekraczania zasobów klastra - wyższe wartości nie zawsze prowadzą do poprawy wydajności.\n",
    "\n",
    "**Walidacja krzyżowa** </br>\n",
    "CrossValidator rozpoczyna pracę od podzielenia zestawu danych na mniejsze zbiory, które są używane jako oddzielne zestawy danych treningowych i testowych. Np. przy k=3, CrossValidator wygeneruje 3 pary zestawów danych (trening, test), z których każdy wykorzystuje 2/3 danych do treningu i 1/3 do testowania. Aby ocenić konkretne wartości parametrów, CrossValidator oblicza średnią metrykę oceny dla tych 3 modeli. Po zidentyfikowaniu najlepszej pary dla ParamMap, CrossValidator ponownie dopasowuje Estimator przy użyciu najlepszej ParamMap używając całego zestawu danych. Poniżej przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.2665, 0.7335]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9204, 0.0796]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.4438, 0.5562]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.8587, 0.1413]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train - test split** </br>\n",
    "Spark oferuje także TrainValidationSplit do dostrajania hiperparametrów. TrainValidationSplit ocenia każdą kombinację parametrów tylko raz. Jest tańszy, ale nie da tak wiarygodnych wyników, gdy zestaw danych szkoleniowych nie jest wystarczająco duży.\n",
    "\n",
    "W przeciwieństwie do CrossValidator, TrainValidationSplit tworzy pojedynczą parę (treningową, testową) zestawu danych. Dzieli zestaw danych na dwie części za pomocą parametru trainRatio. Na przykład z trainRatio=0,75, TrainValidationSplit wygeneruje parę treningową i testową, w której 75% danych jest używanych do szkolenia, a 25% do walidacji.\n",
    "\n",
    "Podobnie jak CrossValidator, TrainValidationSplit w końcu dopasuje Estimatorem model przy użyciu najlepszej mapy ParamMap i całego zestawu danych. Poniżej przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/c:/Users/Marta Głowińska/Desktop/DJL PySpark/warsztaty/data/mllib/sample_linear_regression_data.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Marta Głowińska\\Desktop\\DJL PySpark\\warsztaty\\07. MLib.ipynb Cell 48\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/07.%20MLib.ipynb#Y110sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtuning\u001b[39;00m \u001b[39mimport\u001b[39;00m ParamGridBuilder, TrainValidationSplit\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/07.%20MLib.ipynb#Y110sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Prepare training and test data.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/07.%20MLib.ipynb#Y110sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mlibsvm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/07.%20MLib.ipynb#Y110sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mdata/mllib/sample_linear_regression_data.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/07.%20MLib.ipynb#Y110sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train, test \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mrandomSplit([\u001b[39m0.9\u001b[39m, \u001b[39m0.1\u001b[39m], seed\u001b[39m=\u001b[39m\u001b[39m12345\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/07.%20MLib.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m lr \u001b[39m=\u001b[39m LinearRegression(maxIter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Public\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:158\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[0;32m    159\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Public\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\Public\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/c:/Users/Marta Głowińska/Desktop/DJL PySpark/warsztaty/data/mllib/sample_linear_regression_data.txt"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Prepare training and test data.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"data/mllib/sample_linear_regression_data.txt\")\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4ebc63f2cded5a5017438174b69eeaea6fe35f706fbd25d3a5f5d6de9a88d22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
