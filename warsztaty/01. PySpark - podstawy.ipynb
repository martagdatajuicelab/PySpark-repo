{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co to jest PySpark?\n",
    "\n",
    "PySpark to biblioteka, której można używać do uruchamiania aplikacji Pythona przy użyciu możliwości Apache Spark. Innymi słowy, PySpark to API Pythona dla Sparka. Wiele osób myli Sparka z językiem programowania. Spark też jest biblioteką, której mogą używać języki programowania takie jak Java, Scala, R i Python. Używając PySpark możemy uruchamiać aplikacje równolegle w rozproszonym klastrze. Klaster to zestaw komputerów w jednej sieci. No to po co ten Spark? \n",
    "\n",
    "Zacznijmy najpiew od Apache Spark. Jest to silnik stworzony do wydajnego rozproszonego, wielkoskalowego przetwarzania danych, a także obsługiwania aplikacji uczenia maszynowego. Spark jest odpowiedzią na potrzebę biznesową związaną z przetwarzaniem ogromnych wolumentów danych. Jedną z pierwszych odpowiedzi na tę potrzebę było wynalezienie Hadoopa, a także Mapreduce. Hadoop to zestaw bibliotek zaprojektowanych do działania w klastrze, a Mapreduce to właściwy silnik przetwarzania danych. Problem z Mapreduce polega na tym, że wykonuje on większość operacji na dysku, co skutkowało dłuższym czasem oczekiwania na wynik. Dlatego właśnie w 2009 r. na Uniwersytecie Berkeley wynaleziono Sparka. Spark pracuje na dysku 10 razy szybciej niż Mapreduce i 100 razy szybciej w pamięci. I dlatego właśnie Spark zyskał na popularności\n",
    "\n",
    "# Spark Unified Stack \n",
    "\n",
    "Teraz porozmawiajmy o tzw. Spark Unified Stack. Spark Unified Stack jest zbudowany na Spark Core. Spark Core jest nośnikiem wszystkich funkcje niezbędnych do zarządzania i uruchamiania aplikacji rozproszonych. Mówimy o: planowaniu, koordynacji i odporności na awarie. W pracy ze Sparkiem wykorzystywanych jest kilka modułów i komponentów i każdy z nich jest oparty o Spark Core. Są to: \n",
    "* Spark SQL - ten moduł jest przeznaczony do pracy z danymi ustrukturyzowanymi. Pozwala odczytywać dane z tabel relacyjnej bazy danych lub z CSVek, JSONów, Avro, ORC, a także plików parquet. Można nawet pobierać i zapisywać dane do sparkowje ramki danych. Spark SQL jest zgodny ze standardem ANSI SQL, co oznacza, że działa również SQLengine, umożliwiając pracę przy użyciu składni SQL. \n",
    "* Spark Structured Streaming - umożliwia przetwarzanie strumieniowych danych w czasie rzeczywistym z różnych źródeł z wysoką przepustowością i odpornością na błędy. Dane mogą być pobierane z Kafki, Flume'a, Twitter'a itd. i przetwarzane w czasie rzeczywistym. \n",
    "* Spark MLlib - moduł do uczenia maszynowego. W jego skład wchodzi wiele popularnych algorytmów uczenia maszynowego, które pozwalają budować pipeline'y do uczenia i oceny modeli.  \n",
    "* Spark GraphX - moduł do tworzenia grafów reprezentujących sieci, takich jak LinkedIn.\n",
    "\n",
    "# Spark web UI\n",
    "\n",
    "Apache Spark zapewnia interfejs sieciowy do monitorowania stanu wykonywanej w klastrze aplikacji. Żeby to zrozumieć najlepiej spojrzeć na przykład. Stworzymy rdd, które przyjmie listę liczb. Każdą z tych liczb podniesiemy do kwadratu przy wykorzystaniu wyrażenia lambda. Tworzymy rdd, więc musimy użyć funkcji (a dokładniej: transofrmacji) specyficznej dla RDD: *map()*. Mapowania używa się do złożonych operacji na danych. \n",
    "Zaczniemy do inicjalizacji sesji: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-232E4IU.home:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ReadWriteVal</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29c830a9070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Może chwilę potrwać\n",
    "spark = SparkSession.builder.appName(\"ReadWriteVal\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz stworzymy ramkę:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "num_list = [1,2,3,4]\n",
    "rdd = spark.sparkContext.parallelize(num_list) # o funkcji parallelize() porozmawiamy potem\n",
    "squared = rdd.map(lambda x: x**2).collect() # o funkcji collect() porozmawiamy potem\n",
    "\n",
    "#sprawdźmy czy wszystko ok\n",
    "for num in squared: \n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# podejrzyjmy typ obiektu: \n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz uruchomimy interfejs poprzez kliknięcie w tekst Spark UI, który widoczny jest tuż pod miejscem, w którym zainicjalizowaliśmy sesję. Pierwsza rzecz, która rzuca się w oczy to to, że interfejs użytkownika Spark jest podzielony na kilka zakładek: Jobs, Stages, Storage, Environement, Executors, SQL. Zwróc uwagę na nazwę aplikacji, jest zgodna z tym co jej przypisaliśmy (lub nie) na etapie inicjalizacji sesji. To, co jest ważny to to, że każdy program, który napiszesz, będzie miał przypisany własny adres URL.  \n",
    "\n",
    "Wróćmy do sekcji Jobs. Można w nim zdefiniować czy podejrzeć tryb planowania, liczbę zadań Spark, liczbę etapów zadania oraz opisy zadań. Jeśli chodzi o planowanie dostępne są trzy tryby planowania: autonomiczny (FIFO: first in, first out). Drugi tryb **YARN**. Trzeci to tryb **Mesos**. Przy planowaniu zadań, które nie będą wykorzystywać przetwarzania rozproszonego (tak jak my teraz, na warsztatach) YARN i Mesos są niepotrzebne. Działając lokalnie będziemy wykorzystywać tylko tryb FIFO. \n",
    "\n",
    "Rzućmy teraz okiem na spójrz na Spark Jobs. Do tej pory wykonaliśmy tylko jednego Joba. Spark Job to liczba **akcji** w aplikacji. Będziemy mówić o **akcjach i transformacjach** trochę później. To, co teraz należy wiedzieć to to, że są to dwa całkowicie odrębne procesy i różnice między nimi, z punktu widzenia ewaluacji kodu Sparka, są bardzo istotne. W UI widać tylko **akcję** *collect()*., o której wiemy, że poprzedziliśmy ją **transformacją** *map()*. Dla Sparka jest to pojedynczy etap wykonywanego zadania (Job). Każdy Job w Sparku powinien mieć przynajmniej jeden etap. Jednak nie wszystkie operacje będą się kończyły na jednym. W rzeczywistości zadanie może mieć jeden lub więcej etapów. Etapy w Sparku powstają zależnie od tego czy operacje można wykonywać szeregowo czy równolegle. Każdy etap może mieć jedno lub więcej mniejszych zadań (task). Etapy mogą reprezentować pojedynczy element pracy, którą musimy wykonać. Przykładowo - jeden etap mógłby reprezentować połączenie z bazą danych, inny reprezentowałby tasowanie danych i tak dalej. Gdybyśmy wykonali więcej działań pojawiłyby się w tej sekcji. \n",
    "\n",
    "Teraz spójrzmy na oś czasu. Unaocznia ona flow zadania - informuje o momencie inicjalizacji drivera i wykonania *collect()*. Oś czasu dostępna jest dla nas tak długo jak długo trwa dana sesja Sparka. \n",
    "\n",
    "Etapy (stages) - ta sekcja nam bardziej szczegółowy widok etapów wykonywanego zadania. Jeżeli klikniemy Description uzyskamy bardziej szczegółowy opis danego etapu (podrzędne w stosunku do etapów są taski).  Pierwszą rzeczę, którą zobaczymy jest wizualizacja DAG. DAG oznacza Directed Acycle Graph. Opisuje on metodologię wykonania zadań przez Sparka.  W tym momencie nie ma sensu mocno zagłębiać się w DAGi, ponieważ bierząca aplikacja jest po prostu zbyt prosta. \n",
    "\n",
    "Teraz rzućmy okiem na **Aggregated metrics by executor**, które znajduje się pod DAGami. Aby zrozumieć executory, wróćmy do Stage'y. Stage może utworzyć jeden lub więcej task. Każdy task jest odrębną jednostką. Taka jednostka jest przekazywana/mapowana do executora (węzła), który to zadanie wykona. W przypadku naszej prostej aplikacji, widać, że mamy jeden executor, który utworzył co najmniej osiem zadań do równoległego wykonywania. Osiem zadań odpowiada liczbie rdzeni procesora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Execution Concepts\n",
    "\n",
    "Zanim przejdziemy do podstawowego obiektu Sparka, jakim są RDDsy, przytoczymy podstawowe informacje o tym, jak Spark wykonuje polecenia. Poznamy aplikacje i sesje sparkowe. Skonfigurujemy nowy projekt, w którym stworzymy aplikację obliczającą największą ilość zamówień w kraju i w regionie. A potem przejdziemy do rozmów o transformacjach i akcjach Sparka. Potem pokażemy jak interpretować DAGi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na początek zaczniemy od stworzenia nowej sesji i importu funkcji sqlowej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-232E4IU.home:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TotalOrders</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x218a9ef1be0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TotalOrders\").getOrCreate() # Może być tylko jedna sesja Spark per JVM. \n",
    "# Sesja sparka to instancja Javy, bo Spark został napisany w Javie\n",
    "# builder to klasa, a klasy dają dotęp do funkcji charakterystycznych dla klasy \n",
    "# appName = funkcja przyjmuje tylko jeden parametr\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdecydowaliśmy się na użycie Spark SQL ze względu na to, że łatwo jest się nim posługiwać, dużo łatwiej niż. Ponadto SparkSQL zawiera w sobie proste do zrozumienia funkcje sparkowe. \n",
    "\n",
    "Jednak najważniejszą rzeczą, którą powinniście wynieść z tej seksji jest to, że fundamentem każdej aplikacji sparkowej jest Spark Driver, który tworzy obiekt SparkSession. Driver ułatwia także komunikację między sesją, wykorzystywanym notebookiem a executorami. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jesteśmy coraz bliżej zrozumienia w jaki sposób Spark wykonuje polecenia. Nim to nastąpi musimy załadować dane, żeby mieć na czym pracować.\n",
    "\n",
    "Poniżej przykład jak wczytywać dane z plików csv. Parametr *inferSchema* ustawiony na true informuje Sparka, że ma wczytać dane w tej samej postaci, w której zostały zapisane. Parametr *header* ustawiony na true oznacza, że pierwszy wiersz wczytywanego pliku jest nagłówkiem ramki, którą właśnie tworzymy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.read.csv(\"sales_records.csv\", inferSchema=True, header=True)\n",
    "# opcja alternatywna:\n",
    "# sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"sales_df\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podjerzyjmy sobie 10 wierszych wierszy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "|              Region|             Country|    Item Type|Sales Channel|Order Priority|Order Date| Order ID| Ship Date|Units Sold|Unit Price|Unit Cost|Total Revenue|Total Cost|Total Profit|\n",
      "+--------------------+--------------------+-------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "|Middle East and N...|          Azerbaijan|       Snacks|       Online|             C| 10/8/2014|535113847|10/23/2014|       934|    152.58|    97.44|    142509.72|  91008.96|    51500.76|\n",
      "|Central America a...|              Panama|    Cosmetics|      Offline|             L| 2/22/2015|874708545| 2/27/2015|      4551|     437.2|   263.33|    1989697.2|1198414.83|   791282.37|\n",
      "|  Sub-Saharan Africa|Sao Tome and Prin...|       Fruits|      Offline|             M| 12/9/2015|854349935| 1/18/2016|      9986|      9.33|     6.92|     93169.38|  69103.12|    24066.26|\n",
      "|  Sub-Saharan Africa|Sao Tome and Prin...|Personal Care|       Online|             M| 9/17/2014|892836844|10/12/2014|      9118|     81.73|    56.67|    745214.14| 516717.06|   228497.08|\n",
      "|Central America a...|              Belize|    Household|      Offline|             H|  2/4/2010|129280602|  3/5/2010|      5858|    668.27|   502.54|   3914725.66|2943879.32|   970846.34|\n",
      "|              Europe|             Denmark|      Clothes|       Online|             C| 2/20/2013|473105037| 2/28/2013|      1149|    109.28|    35.84|    125562.72|  41180.16|    84382.56|\n",
      "|              Europe|             Germany|    Cosmetics|      Offline|             M| 3/31/2013|754046475|  5/3/2013|      7964|     437.2|   263.33|    3481860.8|2097160.12|  1384700.68|\n",
      "|Middle East and N...|              Turkey|       Fruits|       Online|             C| 3/26/2012|772153747|  4/7/2012|      6307|      9.33|     6.92|     58844.31|  43644.44|    15199.87|\n",
      "|              Europe|      United Kingdom|       Snacks|       Online|             H|12/29/2012|847788178| 1/15/2013|      8217|    152.58|    97.44|   1253749.86| 800664.48|   453085.38|\n",
      "|                Asia|          Kazakhstan|    Cosmetics|       Online|             H| 9/11/2015|471623599| 9/18/2015|      2758|     437.2|   263.33|    1205797.6| 726264.14|   479533.46|\n",
      "+--------------------+--------------------+-------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show(10) #to wygląda brzydko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skrypt poniżej również pozwoli na podejrzenie danych, ale wynik będzie lepiej ułożony:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+---------------------+---------+\n",
      "|Region                           |Country              |Order ID |\n",
      "+---------------------------------+---------------------+---------+\n",
      "|Middle East and North Africa     |Azerbaijan           |535113847|\n",
      "|Central America and the Caribbean|Panama               |874708545|\n",
      "|Sub-Saharan Africa               |Sao Tome and Principe|854349935|\n",
      "|Sub-Saharan Africa               |Sao Tome and Principe|892836844|\n",
      "|Central America and the Caribbean|Belize               |129280602|\n",
      "|Europe                           |Denmark              |473105037|\n",
      "|Europe                           |Germany              |754046475|\n",
      "|Middle East and North Africa     |Turkey               |772153747|\n",
      "|Europe                           |United Kingdom       |847788178|\n",
      "|Asia                             |Kazakhstan           |471623599|\n",
      "+---------------------------------+---------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select(\"Region\", \"Country\", \"Order ID\").show(n=10, truncate= False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upewnijmy się, że to ramka sparkowa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sales_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz wykonamy grupowanie, agregację i zliczanie - chcemy się dowiedzieć, które państwa w obrębie danych regionów miały największą liczbę zamówień: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-------------+------------+\n",
      "|Region                           |Country      |Total Orders|\n",
      "+---------------------------------+-------------+------------+\n",
      "|Sub-Saharan Africa               |Sudan        |623         |\n",
      "|Australia and Oceania            |New Zealand  |593         |\n",
      "|Europe                           |Vatican City |590         |\n",
      "|Europe                           |Malta        |589         |\n",
      "|Sub-Saharan Africa               |Mozambique   |589         |\n",
      "|Middle East and North Africa     |Tunisia      |584         |\n",
      "|Asia                             |Cambodia     |584         |\n",
      "|Central America and the Caribbean|Panama       |578         |\n",
      "|Sub-Saharan Africa               |Rwanda       |576         |\n",
      "|Sub-Saharan Africa               |Cote d'Ivoire|575         |\n",
      "+---------------------------------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total rows:  185\n"
     ]
    }
   ],
   "source": [
    "count_sales_df = (sales_df.select(\"Region\", \"Country\", \"Order ID\")).groupBy(\"Region\", \"Country\").agg(count(\"Order ID\").alias(\"Total Orders\")).orderBy(\"Total Orders\", ascending = False)\n",
    "\n",
    "count_sales_df.show(10, False)\n",
    "print(\"Total rows: \", count_sales_df.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Akcje i transformacje\n",
    "\n",
    "Teraz możemy w końcu przejść do **akcji** i **transformacji**. Spark ma dwa rodzaje transformacji, wąskie i szerokie. \n",
    "Wąska transofmracja jest wtedy, kiedy dane wyjściowe można przeliczyć korzystając tylko z jednej partycji. Partycja, czyli węzeł w klastrze. Dane mogą być rozdystrybuowane po 200 rekordów w jednej partycji, 300 w innej itd. Korzystanie z jednej partycji oznacza, że nie musimy tasować danych zawartych na innych partycjach żeby otrzymać oczekiwany rezultat. \n",
    "Rzućmy okiem na kawałek kodu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list = range(1000)\n",
    "rdd = spark.sparkContext.parallelize(big_list,2)\n",
    "odds = rdd.filter(lambda x:x % 2 !=0)\n",
    "odds.take(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonaliśmy filtrowanie wyników, co jest wąską transformacją. Przefiltrowana zostanie każda partycja osobno i każda osobno zwróci wynik. Dopiero na późniejszym etapie zostaną połączone w jeden zbiór. Partycje nie musiały wymieniać się danymi żeby zrealizować polecenie. \n",
    "\n",
    "A co z GroupBuy i OrderBy? GroupBy i OrderBy są przekształceniami szerokimi - zmuszają driver do przegrupowania danych między węzłami. \n",
    "\n",
    "Powyższa ramka przechowuje dane w dwóch partycjach. Żeby wykonać *groupBy()* czy *orderBy()* dane musiałyby zostać przetasowane żeby móc wykonać to polecenie. Po wykonaniu polecenia zostałyby zebrane do pojedynczego zbioru. Tasowanie danych to działanie mające na celu połączenie danych i ułożenie ich w nową partycję. \n",
    "\n",
    "Transformacje zawsze skutkują nową ramką danych, akcje nie. Ta informacja będzie nam potrzebna w kolejnej części: interpretacji DAGów. \n",
    "\n",
    "# Wizualizacje DAG\n",
    "\n",
    "Witajcie dobrzy ludzie i witajcie z powrotem. W porządku, nadszedł czas, aby ponownie przyjrzeć się interfejsowi Spark Web, a następnie omówić ukierunkowany wykres acykliczny, aby uzyskać jasne wyobrażenie o tym, jak faktycznie działa silnik Spark. Możemy zacząć od odświeżenia naszego notatnika i następnie załaduj interfejs sieciowy Spark. Aby to zrobić, ponownie wybiorę „Jądro” i wybierz Uruchom ponownie i uruchom wszystko, tak po prostu. I zaczekajmy, aż zakończy wykonywanie. [Brak dźwięku] W porządku, więc wydaje się, że wykonanie zostało zakończone, a co możemy zrobić, to po prostu załadować localhost i 4040 i nacisnąć enter. Idealnie. [Brak dźwięku] W porządku. Teraz, jak widać, w przeciwieństwie do wcześniej, mamy teraz więcej rzeczy do zrobienia. Nasza aplikacja ma wiele zadań, jak widać, wiele etapów i wiele zadań, które trwają do około 200 znaków. Jednak to, co nas interesuje dla tego wykładu jest to, co znajdziemy w zakładce Etapy. Wybierzmy więc z menu zakładkę Etapy, a więc wybierz Etapy. Teraz, jak widać, mamy co najmniej osiem etapów i wspomniałem z poprzednia lekcja, że każdy etap reprezentuje jednostkę pracy. Nie pozwól, aby nazewnictwo etapów w Javie cię zniechęciło, nawet jeśli napisałeś kod w Pythonie. Należy zrozumieć, że Spark jest napisany skalarnie, który następnie kompiluje się do kodu bajtowego Javy. Twórcy Sparka zapewnili to, udostępniając API Pythona dla Sparka. Ale za kulisami dzieje się tak, że Twój kod Pythona faktycznie uzyskuje dostęp do API Javy poprzez biblioteki Py4J, które zainstalowałeś podczas instalacji PySpark. oparte na nazewnictwie etapów, aby zobaczyć Py4J w akcji wybierz łącze szczegółów na dowolnym etapie, a ja zrobię to na etapie zero, tak po prostu. I tutaj widzimy Py4J w działaniu komunikujący się z interfejsem API Java. Należy więc pamiętać o jednej rzeczy, te etapy mogą działać równolegle lub szeregowo w zależności od jednostki pracy. Od razu widać, że etapy opowiadają historię naszego programu. Był czas, kiedy potrzebowaliśmy załadować dane, był czas, kiedy potrzebowaliśmy agregacji używając funkcji konta, a na koniec musieliśmy wydrukować dane wyjściowe. W ostatniej lekcji mówiliśmy o transformacjach i akcjach Spark. Spark ma model leniwego wykonywania. Zasadniczo oznacza to, że piszesz transformacje, ale one faktycznie działają dopóki nie wywołasz akcji. Przyjrzyjmy się naszemu programowi. Właściwie możesz zobaczyć liczbę przekształceń, takich jak Select, GroupBy i OrderBy. Jednak w prawdziwie leniwym modelu Spark, nie są one wykonywane do czasu wywołania akcji, takiej jak podczas wykonywania programu Show and Count. Każda akcja zostanie uruchomiona zadanie Spark, a każde zadanie uruchomi kilka etapów i tak dalej. Zacznijmy więc od etapu zerowego i zobaczmy jednostki pracy. Zamierzam więc wybrać [Brak dźwięku] opis etapu zerowego, [Brak dźwięku] i Zamierzam rozszerzyć wizualizację DAG tutaj. DAG pozwala więc zobaczyć, jak jednostka pracy została wykonana w formie wizualnego wykresu. Tutaj widzimy skanowanie pliku, które miało miejsce, gdy załadowaliśmy plik CSV zawierający dane sprzedaży. Następnie następna część pracy polega na tym, że Spark załadował dane do RDD. Pamiętaj, że ramka danych to RDD pod spodem, ale to nie mówi całej historii. Wróćmy do zadań, wybierając powiązane zadanie z tym etapem. Teraz, aby wybrać powiązane zadania, wystarczy przewinąć z powrotem tutaj i po prostu wybrać to zadanie zero, i pozwolić mi ponownie rozwinąć wizualizację DAG. Teraz widzimy tutaj, że zaraz po zeskanowaniu CSV, Spark wywołuje inną akcję, która nazywa się WholeStageCodegen.Teraz jest to generator kodu, który konwertuje kod Spark SQL na kod bajtowy Java e w celu poprawy wydajności wykonywania w klastrze. I wtedy tylko kod zapisze plik CSV w ramce danych lub być może RDD, który jest zwykle podzielony na partycje. Oczywiście ponownie odwiedzimy DAG na późniejszym etapie. teraz zapoznaj się ze szczegółami i dowiedz się więcej. To tyle ode mnie i do zobaczenia w następnym. Do widzenia. [Brak dźwięku]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4ebc63f2cded5a5017438174b69eeaea6fe35f706fbd25d3a5f5d6de9a88d22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
