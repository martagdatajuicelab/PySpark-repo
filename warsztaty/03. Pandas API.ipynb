{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas API\n",
    "\n",
    "źródło: https://www.sicara.fr/blog-technique/run-pandas-code-on-spark, \n",
    "\n",
    "\n",
    "Pandas to bardzo potężna biblioteka, którą znają wszyscy analitycy danych, ale kod Pandas może działać tylko na jednej jednostce. W związku z tym jeżeli przetwarzamy duży zestaw danych za pomocą pandas będzie się to działo bardzo wolno i najprawdopodobniej pojawi się OOM error.\n",
    "\n",
    "Zwykle wtedy do gry wchodzi Spark. PySpark co prawda zawiera  moduł o nazwie Spark SQL, który zapewnia obiekty typu DataFrame podobny do ramek pandas, ale mają one wady:\n",
    "- napisany wcześniej kod pandas nie może zostać poniewnie użyty, ponieważ Pandas nie jest kompatybilny z PySpark DataFrames.\n",
    "- składnia PySpark bardzo różni się od składni Pandas (patrz poniżej), co utrudnia PySparka osobom pracującyjm wcześniej w pandas\n",
    "\n",
    "Poniżej przykład różnic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#PySpark:\\n\\ndf = spark.read.option(\"inferSchema\", True).cvs(\"data.csv\")\\ndf = df.toDF(\"x\",\"y\", \"z\")\\ndf = df.withColumn(\"x2\", df.x * df.x)\\n\\n#Pandas: \\ndf = pd.read_csv(\"data.csv\")\\ndf.columns = [\"x\",\"y\",\"z\"]\\ndf[\"x2\"] = df.x * df.x\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Może chwilę potrwać\n",
    "spark = SparkSession.builder.appName(\"UczymySięSparka\").getOrCreate()\n",
    "spark\n",
    "\n",
    "'''\n",
    "#PySpark:\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True).cvs(\"data.csv\")\n",
    "df = df.toDF(\"x\",\"y\", \"z\")\n",
    "df = df.withColumn(\"x2\", df.x * df.x)\n",
    "\n",
    "#Pandas: \n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df.columns = [\"x\",\"y\",\"z\"]\n",
    "df[\"x2\"] = df.x * df.x\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24 kwietnia 2019 r. firma Databricks ogłosiła nowy projektopen source o nazwie Koalas, którego celem było udostępnienie interfejsu API Pandas na platformie Spark. Biblioteka nabrała rozpędu i została oficjalnie połączona z PySpark w Spark 3.2 (październik 2021 r.) i nazwana API Pandas on Spark.\n",
    "\n",
    "Interfejs API Pandas ma taką samą składnię jak Pandas, ale \"pod spodem\" używa ramek PySparkowych. Oznacza to, że kod napisany za pomocą Pandas API może być uruchamiany w systemie master - slave, w których Spark jest skonfigurowany (w przeciwieństwie do Pandas), co pozwala na obsługę dużych zbiorów danych. Korzystająz z Pandas API można robić prawie wszystko, co z ramkami Pandowymi (~83% funkcji dostępnych w pyspark.pandas). Pełna lista funkcji: \n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python//reference/pyspark.pandas/general_functions.html\n",
    "\n",
    "### Co zrobić, jeśli nie mogę znaleźć funkcji Pandas w Pandas API?\n",
    "\n",
    "Ponieważ ramka danych Pandas-on-Spark wykorzystuje ramkę danych PySpark \"pod spodem\", można ją przekonwertować z/do ramki danych PySpark. Dlatego jeśli nie możesz znaleźć potrzebnej funkcji, nadal możesz wykonać następujące czynności:\n",
    "\n",
    "* przekształcić ramkę danych Pandas-on-Spark w ramkę danych PySpark\n",
    "* wykonać tyle transformacji ile potrzeba za pomocą PySpark\n",
    "* przekonwertować ramkę danych PySpark z powrotem na ramkę danych Pandas-on-Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "psdf = ps.range(10)\n",
    "sdf = psdf.to_spark().filter(\"id > 5\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   6\n",
       "1   7\n",
       "2   8\n",
       "3   9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.to_pandas_on_spark()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej więcej przykładów jak wymiennie korzystać z PySparka i pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps # przekstzałcenie na ramkę pandasową\n",
    "psdf = ps.range(10)\n",
    "pdf = psdf.to_pandas()\n",
    "pdf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   0\n",
       "1   1\n",
       "2   2\n",
       "3   3\n",
       "4   4\n",
       "5   5\n",
       "6   6\n",
       "7   7\n",
       "8   8\n",
       "9   9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.from_pandas(pdf) # pandas on spark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "psdf = ps.range(10)\n",
    "sdf = psdf.to_spark().filter(\"id > 5\") ## ramki sprakowe i pandasowe są do siebie bardzo podobne, tu przekształcenie z ramki pandasowej na sparkową\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   6\n",
       "1   7\n",
       "2   8\n",
       "3   9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.to_pandas_on_spark() ## ze sparka do pandas on spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jak działa Pandas API?\n",
    "Gdy użytkownik tworzy ramkę danych Pandas-on-Spark, powstaje wtedy też „ramka wewnętrzna” i ramka sparkowa.\n",
    "\n",
    "„Ramka wewnętrzna” zapewnia konwersje między ramkami pandas-on-Spark i PySpark. Przechowuje metadane, takie jak mapowanie danych w kolumnach i indeksów.\n",
    "\n",
    "Pozwala ramce Pandas-on-Spark na obsługę funkcji Pandas, które nie są obsługiwane przez ramki PySpark:\n",
    "\n",
    "* mutable syntax: dzięki czemu nie trzeba tworzyć nowej ramki za każdym razem, gdy chcemy coś zmodyfikować\n",
    "* indeks sekwencyjny: żeby można było manipulować ramką w oparciu o indeks (patrz poniżej)\n",
    "* pandowe typy danych\n",
    "\n",
    "Należy pamiętać, że dane są rozproszone po wielu workerach, podczas gdy w Pandzie dane pozostają na jednej maszynie.\n",
    "\n",
    "### Indeksowanie\n",
    "\n",
    "W przeciwieństwie do ramki PySpark, ramki Pandas-on-Spark replikują funkcjonalność indeksowania Pandas (dzięki wspomianej wyżej wewnętrznej ramce). Dla przypomnienia, indeksy służą do uzyskiwania dostępu do wierszy przez indeksatory loc/iloc lub do mapowania właściwych wierszy w przypadku operacji łączących dwie ramki danych lub serie (na przykład df1 + df2) i tak dalej.\n",
    "\n",
    "Jeśli żadna z kolumn nie została określona jako indeks zostanie użyty indeks domyślny. Może to być jeden z 3:\n",
    "\n",
    "**Sekwencja** </br>\n",
    "Używany domyślnie. Implementuje sekwencję, która zwiększa się o jeden razem z każdym kolejnym rekordem. Najprawdopodobniej spowoduje to przeniesienie całej ramki do jednego klastra, co będzie bardzo powolne i najprawdopodoniej rzuci OOM error. Nie należy go używać, gdy zbiór danych jest duży."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2], dtype='int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.default_index_type', 'sequence')\n",
    "psdf = ps.range(3)\n",
    "ps.reset_option('compute.default_index_type')\n",
    "psdf.index\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rozproszona sekwencja** </br>\n",
    "Mechanizm jest ten sam, co wyżej, ale indeks rozproszony można wykorzystywać razem z partycjonowaniem. Powinien zostać użyty jeśli zbiór  danych jest duży i potrzebny jest indeks sekwencyjny. Należy zauważyć, że jeśli po utworzeniu tego indeksu do zbioru danych zostanie dodanych więcej rekordów nie ma gwarancji, że indeks pozostanie sekwencyjny.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2], dtype='int64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.default_index_type', 'distributed-sequence')\n",
    "psdf = ps.range(3)\n",
    "ps.reset_option('compute.default_index_type')\n",
    "psdf.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rozlokowany (distributed)** </br>\n",
    "Implementuje ciąg rosnący monotonicznie, ale nie nieprzerwany (np. 1, 8, 12), wartości są przypadkowe. Nie ma nic wspólnego z indeksowaniem Pandas. Pod względem wydajności jest najlepszy, ale nie można go używać do wykonywania operacji na dwóch ramkach. \n",
    "\n",
    "Poniżej przykład ustawiania typu indeksu, z któego chcemy skorzystać: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([42949672960, 85899345920, 128849018880], dtype='int64')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.default_index_type', 'distributed')\n",
    "psdf = ps.range(3)\n",
    "ps.reset_option('compute.default_index_type')\n",
    "psdf.index\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pozostałe opcje\n",
    "\n",
    "Pandas API ma system opcji, który pozwala dostosować niektóre aspekty jego pracy. Najczęściej użytkownicy zmieniają opcję wyświetlania wyników. Najważniejsze w tym wypadku są dwa polecenia: \n",
    "\n",
    "* get_option() / set_option() - podejrzenie/ustawienie opcji\n",
    "* reset_option() - zresteowanie danej opcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.get_option('compute.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.set_option('compute.max_rows', 2000)\n",
    "ps.get_option('compute.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.reset_option(\"display.max_rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.get_option('compute.max_rows')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operacje na różnych ramkach\n",
    "Pandas API domyślnie blokuje operacje na różnych ramkach (lub seriach), aby zapobiec zbyt ciężkim obliczeniowo operacjom. \n",
    "\n",
    "Można to włączyć, ustawiając compute.ops_on_diff_frames na True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id\n",
       "0 -5.0\n",
       "1 -3.0\n",
       "2 -1.0\n",
       "3  NaN\n",
       "4  NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.ops_on_diff_frames', True)\n",
    "psdf1 = ps.range(5)\n",
    "psdf2 = ps.DataFrame({'id': [5, 4, 3]})\n",
    "(psdf1 - psdf2).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.reset_option('compute.ops_on_diff_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>new_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  new_col\n",
       "0   0      1.0\n",
       "1   1      2.0\n",
       "2   2      3.0\n",
       "3   3      4.0\n",
       "4   4      NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.ops_on_diff_frames', True)\n",
    "psdf = ps.range(5)\n",
    "psser_a = ps.Series([1, 2, 3, 4])\n",
    "psdf['new_col'] = psser_a\n",
    "psdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wszystkie dostępne opcje można sprawdzić pod tym linkiem: https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#available-options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4ebc63f2cded5a5017438174b69eeaea6fe35f706fbd25d3a5f5d6de9a88d22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
