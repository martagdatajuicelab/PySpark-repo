{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs - wprowadzenie\n",
    "\n",
    "RDD czyli Resilient Distributed Dataset. RDD API zostało wydane w 2011 r, co oznacza, że od powstania Sparka to RDD API było głównym interfejsem, z którego mogli korzystać użytkownicy Sparka. API to Application Programmers Interface i jest to w zasadzie zestaw bibliotek, które są udostępniane w celu rozszerzenia funkcjonalności lub możliwości określonego języka programowania. Na dzień dzisiejszy możemy RDD traktować jako API (jeśli decydujemy się na korzystanie z ramek pysparkowych) albo jak zbiór rozproszonych danych. \n",
    "\n",
    "RDD, jak nazwa wskazuje, jest odpornym, rozproszonym, niezmiennym (immutable) zbiorem danych. Dane są podzielone na węzły w klastrze. RDD jest zawsze definiowany z alokacją danych, a akcje i transformacje w żaden sposób nie zmieniają jego kształtu. Dowolna z nich zawsze zwróci nowy RDD, przechowujący wyniki operacji. Dlatego działania wykonywane na RDD mogą się dziać równolegle. RDD muszą być odporne, ponieważ klastry mogą ulec awarii. RDD to interfejsy API niskiego poziomu, które nie jest łatwo zrozumieć jeżeli do tej pory korzystało się głównie z ustrukturyzowanych interfejsów API wysokiego poziomu, takich jak ramki danych i zbiory danych. Niemniej RDDsy trzeba znać i rozumieć żeby móc sprawnie przekształcać nieustrukturyzowane dane strumieniowe, których nie sposób okiełznać używając wysokopoziomiowych ramek. Innym powodem jest konieczność dogłębnego zrozumienia działania Sparka na wypadek potrzeby zajrzenia pod mechanizmy, które wykorzystują ramki. Zrozumienie działania RDD bardzo pomoże w optymalizacji działania aplikacji Sparkowych. Od czasów Spark 2 (i wprowadzenia wysokopoziomowych API) RDD traktowane są jak relikt przeszłości, ale jest to zgubne podejście. Dlatego powinniśmy się nad nimi pochylić.\n",
    "\n",
    "Mamy za sobą wstęp teoretyczny, pora na kilka linijek praktyki. Wykonamy na RDD kilka prostych operacji korzystając przy tym z podstawowych funkcji. Musimy pamiętać, że pracując na RDDsach pracujemy na obiektach Javowych, które różnią się od ramek sparkowych. Przygotujemy teraz obiekt do modelowania: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestRDD\").getOrCreate()\n",
    "\n",
    "\n",
    "slowa = \"Spark znacznie ułatwia życie i wprawia mnie w dobry nastrój, Spark jest fantastyczny\".split(\" \")\n",
    "type(slowa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark', 'znacznie', 'ułatwia', 'życie', 'i', 'wprawia', 'mnie', 'w', 'dobry', 'nastrój,', 'Spark', 'jest', 'fantastyczny']\n"
     ]
    }
   ],
   "source": [
    "print(slowa) #faktycznie lista"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz musimy stworzyć RDD, które tę listę słów będzie przechowywać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slowa_rdd = spark.sparkContext.parallelize(slowa)\n",
    "type(slowa_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "znacznie\n",
      "ułatwia\n",
      "życie\n",
      "i\n",
      "wprawia\n",
      "mnie\n",
      "w\n",
      "dobry\n",
      "nastrój,\n",
      "Spark\n",
      "jest\n",
      "fantastyczny\n"
     ]
    }
   ],
   "source": [
    "slowa_dane = slowa_rdd.collect() # sprawdzmy czy na pewno wszystko ok\n",
    "# nie da się przepętlić przez RDD - trzeba je zebrać do nowego zbioru\n",
    "for slowo in slowa_dane: \n",
    "    print(slowo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacja distinct \n",
    "\n",
    "Na RDDsach można wykonywać wiele różnych transformacji, my przyjrzymy się tylko kilku podstawowym. Zaczniemy od distincta, który pomoże nam pozbyć się duplikatów. \n",
    "\n",
    "Najpierw trzeba policzyć elementy w RDD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slowa_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slowa_rdd.distinct().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policzyliśmy wszystkie słowa, policzyliśmy unikalne słowa. Ale czy je usunęliśmy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "znacznie\n",
      "ułatwia\n",
      "życie\n",
      "i\n",
      "wprawia\n",
      "mnie\n",
      "w\n",
      "dobry\n",
      "nastrój,\n",
      "Spark\n",
      "jest\n",
      "fantastyczny\n"
     ]
    }
   ],
   "source": [
    "slowa_dane = slowa_rdd.collect() #sprawdzmy czy na pewno wszystko ok\n",
    "for slowo in slowa_dane: \n",
    "    print(slowo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplikaty nadal są, co potwierdza tylko, że dowolna transformacja RDD zwraca nowy zbiór danych, nie ingerując w zawartość zbioru pierwotnego. Żeby móc się realnie pozbyć duplikatów wynik transformacji trzeba zapisać do nowego zbioru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "znacznie\n",
      "wprawia\n",
      "ułatwia\n",
      "dobry\n",
      "i\n",
      "jest\n",
      "mnie\n",
      "Spark\n",
      "życie\n",
      "w\n",
      "fantastyczny\n",
      "nastrój,\n"
     ]
    }
   ],
   "source": [
    "slowa_unique_rdd = slowa_rdd.distinct()\n",
    "slowa_unique_dane = slowa_unique_rdd.collect()\n",
    "\n",
    "for slowo in slowa_unique_dane: \n",
    "    print(slowo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dubli brak. Teraz przefiltrujemy RDD, wybierzemy tylko te słowa, które zaczynają się na literę s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# musimy napisać pod to własną funkcję:\n",
    "\n",
    "def slowa_s(slowo,litera): \n",
    "    return slowo.startswith(litera) # skorzystamy z startswith() dotępne w Pythonie\n",
    "\n",
    "slowa_unique_rdd.filter(lambda slowo: slowa_s(slowo,\"S\")).collect() #filtrując rekordy RDDs najczęściej trzeba będzie to robić korzystając jednocześnie z pythonowych funkcji anonimowych (lambda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacje map i flatMap\n",
    "\n",
    "Transformacja *map()* służy do wykonywania złożonych operacji, takich jak dodawanie kolumny lub zmiany/przekształcenia wartości kolumny. Dane wyjściowe po ww transformacji zawsze będą miały taką samą liczbę rekodów jak dane wejściowe. Przejdżmy do przykładu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "# zaczynamy od stworzenia listy numerów: \n",
    "\n",
    "num_list = [*range(1,21)]\n",
    "print(num_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(2, 4)\n",
      "(3, 9)\n",
      "(4, 16)\n",
      "(5, 25)\n",
      "(6, 36)\n",
      "(7, 49)\n",
      "(8, 64)\n",
      "(9, 81)\n",
      "(10, 100)\n",
      "(11, 121)\n",
      "(12, 144)\n",
      "(13, 169)\n",
      "(14, 196)\n",
      "(15, 225)\n",
      "(16, 256)\n",
      "(17, 289)\n",
      "(18, 324)\n",
      "(19, 361)\n",
      "(20, 400)\n"
     ]
    }
   ],
   "source": [
    "nums_rdd = spark.sparkContext.parallelize(num_list) # tworzymy listę numerów\n",
    "nums_squared_rdd = nums_rdd.map(lambda n: (n, n**2)) # transformujemy ją w listę tupli, które zawierają numer i jego kwadrat\n",
    "for num in nums_squared_rdd.collect(): \n",
    "    print(num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podsumowując - jeśli chcemy przetransformować rdd, możemy to wykonać korzystając z funckji *map()*. Jeszcze jeden przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Spark', 'S', True)\n",
      "('znacznie', 'z', False)\n",
      "('ułatwia', 'u', False)\n",
      "('życie', 'ż', False)\n",
      "('i', 'i', False)\n",
      "('wprawia', 'w', False)\n",
      "('mnie', 'm', False)\n",
      "('w', 'w', False)\n",
      "('dobry', 'd', False)\n",
      "('nastrój,', 'n', False)\n",
      "('Spark', 'S', True)\n",
      "('jest', 'j', False)\n",
      "('fantastyczny', 'f', False)\n"
     ]
    }
   ],
   "source": [
    "slowa_trd_rdd = slowa_rdd.map(lambda word: (word, word[0], slowa_s(word,\"S\")))\n",
    "\n",
    "for trd in slowa_trd_rdd.collect(): \n",
    "    print(trd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformacja *flatMap()* jest rozszerzeniem transformacji *map()*. Możemy spotkać się z sytuacją, kiedy danymi wyjściowymi bedzie np. lista słów, a my z tej listy wyrazów będziemy musieli zrobić jedną, wielką, listę litter. Czyli potrzebujemy zejść poziom niżej niż to, co daje nam *map()*. Rzućmy okiem na poniższy przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'p', 'a', 'r', 'k', 'z', 'n', 'a', 'c', 'z']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slowa_rdd.flatMap(lambda word: list(word)).take(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sortowanie przy użyciu SortBykey()\n",
    "\n",
    "Ostatnia z przykładowych trnsformacji RDDs - sortowanie po kluczu. Na początek przygotujmy dane. Transformacja *SortByKey()* jako input przyjmuje parę klucz - wartość, więc żeby móc wykonać przykład, musimy stworzyć sobie tuplę. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Greece', 13)\n",
      "('India', 91)\n",
      "('USA', 4)\n"
     ]
    }
   ],
   "source": [
    "countries_list = [(\"India\",91), (\"USA\",4), (\"Greece\",13)]\n",
    "countries_rdd = spark.sparkContext.parallelize(countries_list)\n",
    "srt_countries_list = countries_rdd.sortByKey().collect()\n",
    "\n",
    "for country in srt_countries_list:\n",
    "    print(country) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 'India')\n",
      "(13, 'Greece')\n",
      "(4, 'USA')\n"
     ]
    }
   ],
   "source": [
    "srt_countries_list = countries_rdd.map(lambda c: (c[1], c[0])).sortByKey(False).collect()\n",
    "# map() + lambda zamienia miejscami kolumny w liście żeby móc zwrócić coś, co ma sens\n",
    "# false w sortByKey żeby zwrócić wartości od największej do najmniejszej\n",
    "\n",
    "for country in srt_countries_list:\n",
    "    print(country) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4ebc63f2cded5a5017438174b69eeaea6fe35f706fbd25d3a5f5d6de9a88d22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
