{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wprowadzenie do PySpark SQL #\n",
    "\n",
    "źródło: https://sparkbyexamples.com/pyspark/pyspark-sql-with-examples/\n",
    "\n",
    "Pyspark.sql to moduł w PySpark, który służy do wykonywania operacji podobnych do SQL.\n",
    "\n",
    "### PySpark SQL DataFrame API ###\n",
    "\n",
    "DataFrame to rozpartycjonowany zbiór danych zorganizowanych w nazwane kolumny. Koncepcyjnie jest odpowiednikiem tabeli w relacyjnej bazie danych lub ramki danych w R/Python, ale z większą możliwością optymalizacji prędkości zapytań. DataFrames można konstruować z szerokiej gamy źródeł, takich jak pliki z danymi, tabele w Hive, zewnętrzne bazy danych lub istniejące RDD.\n",
    "\n",
    "PySpark DataFrame jest bardzo podobny do Pandas DataFrame, z wyjątkiem tego, że PySpark DataFrames są podzielone na węzły (co oznacza, że dane w DataFrame są przechowywane na różnych jednostkach w klastrze), a wszelkie operacje w PySpark są wykonywane równolegle na wszystkich partycjach, podczas gdy Panda Dataframe przechowuje dane i oblicza wyniki na jednej jednostce.\n",
    "\n",
    "### Uruchamianie zapytań SQL w PySpark ###\n",
    "\n",
    "PySpark SQL jest jednym z najczęściej używanych modułów PySpark. Po stworzeniu ramki można wykonywać na niej operacje przy użyciu składni SQL. Innymi słowy, Spark SQL zapewnia zapytania RAW SQL w Spark, co oznacza, że można korzystać z tradycyjnego ANSI SQL pracując na ramce sparkowej.\n",
    "\n",
    "Aby użyć zapytania SQL najpierw trzeba stworzyć tabelę tymczasową za pomocą funkcji createOrReplaceTempView(). Po wykonaniu tego kroku tabela będzie dostępna dla całej sesji SparkSession po użyciu funkcji sql(). Zostanie usunięta wraz z zakończeniem sesji.\n",
    "\n",
    "Wystarczy użyć metody sql() obiektu SparkSession, aby uruchomić zapytanie, a metoda zwróci nową ramkę.\n",
    "\n",
    "### PySpark SQL przykłady ###\n",
    "\n",
    "**Tworzenie widoku SQL**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|               City|Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|          10|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|       49347|     US|               HOLT|  32564|   FL|\n",
      "|       49348|     US|          HOMOSASSA|  34487|   FL|\n",
      "|       61391|     US|  CINGULAR WIRELESS|  76166|   TX|\n",
      "|       61392|     US|         FORT WORTH|  76177|   TX|\n",
      "|       61393|     US|           FT WORTH|  76177|   TX|\n",
      "|       54356|     US|        SPRUCE PINE|  35585|   AL|\n",
      "|       76511|     US|           ASH HILL|  27007|   NC|\n",
      "|           4|     US|    URB EUGENE RICE|    704|   PR|\n",
      "|       39827|     US|               MESA|  85209|   AZ|\n",
      "|       39828|     US|               MESA|  85210|   AZ|\n",
      "|       49345|     US|           HILLIARD|  32046|   FL|\n",
      "|       49346|     US|             HOLDER|  34445|   FL|\n",
      "|           3|     US|      SECT LANAUSSE|    704|   PR|\n",
      "|       54354|     US|      SPRING GARDEN|  36275|   AL|\n",
      "|       54355|     US|        SPRINGVILLE|  35146|   AL|\n",
      "|       76512|     US|           ASHEBORO|  27203|   NC|\n",
      "|       76513|     US|           ASHEBORO|  27204|   NC|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Może chwilę potrwać\n",
    "spark = SparkSession.builder.appName(\"UczymySięSparka\").getOrCreate()\n",
    "spark\n",
    "\n",
    "df = spark.read.option(\"header\",True) \\\n",
    "          .csv(\"simple-zipcodes.csv\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby korzystać z zapytania ANSI SQL podobnego do zapytań RDBMS, należy utworzyć tymczasową tabelę, odczytując dane z pliku CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"simple-zipcodes.csv\") \\\n",
    "          .createOrReplaceTempView(\"Zipcodes\") ### to tworzy widok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySpark SQL - Select**\n",
    "\n",
    "select() jest wykorzystywany do wyboru konkretnych kolumn z ramki.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------+-----+\n",
      "|country|               city|zipcode|state|\n",
      "+-------+-------------------+-------+-----+\n",
      "|     US|        PARC PARQUE|    704|   PR|\n",
      "|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|     US|               HOLT|  32564|   FL|\n",
      "|     US|          HOMOSASSA|  34487|   FL|\n",
      "+-------+-------------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark: \n",
    "\n",
    "df.select(\"country\",\"city\",\"zipcode\",\"state\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------+-----+\n",
      "|country|               city|zipcode|state|\n",
      "+-------+-------------------+-------+-----+\n",
      "|     US|        PARC PARQUE|    704|   PR|\n",
      "|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|     US|               HOLT|  32564|   FL|\n",
      "|     US|          HOMOSASSA|  34487|   FL|\n",
      "+-------+-------------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT country, city, zipcode, state FROM ZIPCODES\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtrowanie**\n",
    "\n",
    "Żeby przefiltrować ramkę można wykorzystać poniższe sposoby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+-----+\n",
      "|country|city|zipcode|state|\n",
      "+-------+----+-------+-----+\n",
      "|     US|MESA|  85209|   AZ|\n",
      "|     US|MESA|  85210|   AZ|\n",
      "+-------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PySpark:\n",
    "\n",
    "df.select(\"country\",\"city\",\"zipcode\",\"state\").where(\"state == 'AZ'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+-----+\n",
      "|country|city|zipcode|state|\n",
      "+-------+----+-------+-----+\n",
      "|     US|MESA|  85209|   AZ|\n",
      "|     US|MESA|  85210|   AZ|\n",
      "+-------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL where\n",
    "spark.sql(\"\"\" SELECT  country, city, zipcode, state FROM ZIPCODES WHERE state = 'AZ' \"\"\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sortowanie**\n",
    "\n",
    "Żeby posortowac wiersze według wartości konretnej kolumny należy wykorzystać orderBy().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------+-----+\n",
      "|country|               city|zipcode|state|\n",
      "+-------+-------------------+-------+-----+\n",
      "|     US|               MESA|  85209|   AZ|\n",
      "|     US|               MESA|  85210|   AZ|\n",
      "|     US|               HOLT|  32564|   FL|\n",
      "|     US|             HOLDER|  34445|   FL|\n",
      "|     US|          HOMOSASSA|  34487|   FL|\n",
      "|     US|           HILLIARD|  32046|   FL|\n",
      "|     US|        PARC PARQUE|    704|   PR|\n",
      "|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|     US|    URB EUGENE RICE|    704|   PR|\n",
      "+-------+-------------------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"country\",\"city\",\"zipcode\",\"state\").where(\"state in ('PR','AZ','FL')\").orderBy(\"state\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W SQL sortowanie odbywa się za pomocą polecenia ORDER BY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------+-----+\n",
      "|country|               city|zipcode|state|\n",
      "+-------+-------------------+-------+-----+\n",
      "|     US|               MESA|  85209|   AZ|\n",
      "|     US|               MESA|  85210|   AZ|\n",
      "|     US|               HOLT|  32564|   FL|\n",
      "|     US|             HOLDER|  34445|   FL|\n",
      "|     US|          HOMOSASSA|  34487|   FL|\n",
      "|     US|           HILLIARD|  32046|   FL|\n",
      "|     US|        PARC PARQUE|    704|   PR|\n",
      "|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|     US|    URB EUGENE RICE|    704|   PR|\n",
      "+-------+-------------------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT  country, city, zipcode, state FROM ZIPCODES WHERE state in ('PR','AZ','FL') order by state \"\"\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grupowanie**\n",
    "\n",
    "Grupowanie wykonuje się przy użyciu polecenia groupBy(). Często w parze z poleceniem count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   AZ|    2|\n",
      "|   NC|    3|\n",
      "|   AL|    3|\n",
      "|   TX|    3|\n",
      "|   FL|    4|\n",
      "|   PR|    5|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W SQL tę samą operacją przeprowadza się przy użyciu polecenia GROUP BY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   AZ|    2|\n",
      "|   NC|    3|\n",
      "|   AL|    3|\n",
      "|   TX|    3|\n",
      "|   FL|    4|\n",
      "|   PR|    5|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT state, count(*) as count FROM ZIPCODES GROUP BY state\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joiny - PySpark SQL ###\n",
    "\n",
    "PySpark Join służy do łączenia dwóch lub więcej ramek. PySpark obsługuje wszystkie podstawowe operacje łączenia dostępne w tradycyjnym SQL, takie jak INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN. Należy jednak podkreślić, że joiny PySpark to duże transformacje, które obejmują tasowanie danych w obrębie wielu partycji.\n",
    "\n",
    "PySpark SQL Joins z definicji są bardziej optymalne (dzięki właściwościom ramek sparkowych), jednak nadal istnieją pewne problemy z wydajnością, które należy wziąć pod uwagę podczas używania.\n",
    "\n",
    "**Składnia joina:**\n",
    "\n",
    "join(self, other, on=None, how=None)\n",
    "\n",
    "param other: prawa strona joina\n",
    "param on: info po czym łączymy\n",
    "param how: typ joina, argument defaultowy to inner\n",
    "\n",
    "Do joinów można też dodawać where() i filter(). Można też łączyć po więcej niż jednej kolumnie. \n",
    "\n",
    "Nie przerobimy wszystkich typoów joinów, skupimy się na najbardziej podstawowych: inner, left i full outer. W. kolejnym kroku stworzymy ramki do ćwiczeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inner Join**\n",
    "\n",
    "Inner jest domyślnym łączeniem w PySpark i jest najczęściej używanym. To łączy dwa zestawy danych po kolumnach wskazanych jako klucze. Zwróci ramkę, w której znajdą się tylko te wiersze, których klucze sa obecne w obu zbiorach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full Outer Join**\n",
    "\n",
    "Zwraca wszystkie wiersze z obu ramek. Tam, gdzie wiersze nie mogą zostać połączone po kluczu zwracany jest null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left Semi Join**\n",
    "\n",
    "Left semi join w PySpark działa jak tradycyjny left join - zwraca te wiersze, z lewej tabeli, które można połączyć po kluczu z wierszami z prawej tabeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4ebc63f2cded5a5017438174b69eeaea6fe35f706fbd25d3a5f5d6de9a88d22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
