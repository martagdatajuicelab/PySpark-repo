{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas API\n",
    "\n",
    "źródło: https://www.sicara.fr/blog-technique/run-pandas-code-on-spark, </br> \n",
    "https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html </br>\n",
    "https://sparkbyexamples.com/pyspark/pandas-api-on-apache-spark-pyspark/ </br>\n",
    "\n",
    "Pandas to bardzo potężna biblioteka, którą znają wszyscy analitycy danych, ale kod Pandas może działać tylko na jednej jednostce. W związku z tym jeżeli przetwarzamy duży zestaw danych za pomocą pandas będzie się to działo bardzo wolno i najprawdopodobniej pojawi się OOM error.\n",
    "\n",
    "Zwykle wtedy do gry wchodzi Spark. PySpark co prawda zawiera  moduł o nazwie Spark SQL, który zapewnia obiekty typu DataFrame podobny do ramek pandas, ale mają one wady:\n",
    "- napisany wcześniej kod pandas nie może zostać poniewnie użyty, ponieważ Pandas nie jest kompatybilny z PySpark DataFrames.\n",
    "- składnia PySpark bardzo różni się od składni Pandas (patrz poniżej), co utrudnia PySparka osobom pracującyjm wcześniej w pandas\n",
    "\n",
    "Poniżej przykład różnic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#PySpark:\\n\\ndf = spark.read.option(\"inferSchema\", True).cvs(\"data.csv\")\\ndf = df.toDF(\"x\",\"y\", \"z\")\\ndf = df.withColumn(\"x2\", df.x * df.x)\\n\\n#Pandas: \\ndf = pd.read_csv(\"data.csv\")\\ndf.columns = [\"x\",\"y\",\"z\"]\\ndf[\"x2\"] = df.x * df.x\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Może chwilę potrwać\n",
    "spark = SparkSession.builder.appName(\"UczymySięSparka\").getOrCreate()\n",
    "spark\n",
    "\n",
    "'''\n",
    "#PySpark:\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True).cvs(\"data.csv\")\n",
    "df = df.toDF(\"x\",\"y\", \"z\")\n",
    "df = df.withColumn(\"x2\", df.x * df.x)\n",
    "\n",
    "#Pandas: \n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df.columns = [\"x\",\"y\",\"z\"]\n",
    "df[\"x2\"] = df.x * df.x\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24 kwietnia 2019 r. firma Databricks ogłosiła nowy projektopen source o nazwie Koalas, którego celem było udostępnienie interfejsu API Pandas na platformie Spark. Biblioteka nabrała rozpędu i została oficjalnie połączona z PySpark w Spark 3.2 (październik 2021 r.) i nazwana API Pandas on Spark.\n",
    "\n",
    "Interfejs API Pandas ma taką samą składnię jak Pandas, ale \"pod spodem\" używa ramek PySparkowych. Oznacza to, że kod napisany za pomocą Pandas API może być uruchamiany w systemie master - slave, w których Spark jest skonfigurowany (w przeciwieństwie do Pandas), co pozwala na obsługę dużych zbiorów danych. Korzystająz z Pandas API można robić prawie wszystko, co z ramkami Pandowymi (~83% funkcji dostępnych w pyspark.pandas). Pełna lista funkcji: \n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python//reference/pyspark.pandas/general_functions.html\n",
    "\n",
    "### Co zrobić, jeśli nie mogę znaleźć funkcji Pandas w Pandas API?\n",
    "\n",
    "Ponieważ ramka danych Pandas-on-Spark wykorzystuje ramkę danych PySpark \"pod spodem\", można ją przekonwertować z/do ramki danych PySpark. Dlatego jeśli nie możesz znaleźć potrzebnej funkcji, nadal możesz wykonać następujące czynności:\n",
    "\n",
    "* przekształcić ramkę danych Pandas-on-Spark w ramkę danych PySpark\n",
    "* wykonać tyle transformacji ile potrzeba za pomocą PySpark\n",
    "* przekonwertować ramkę danych PySpark z powrotem na ramkę danych Pandas-on-Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "psdf = ps.range(10)\n",
    "sdf = psdf.to_spark().filter(\"id > 5\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   6\n",
       "1   7\n",
       "2   8\n",
       "3   9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.to_pandas_on_spark()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej więcej przykładów jak wymiennie korzystać z PySparka i pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps # przekstzałcenie na ramkę pandasową\n",
    "psdf = ps.range(10)\n",
    "pdf = psdf.to_pandas()\n",
    "pdf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   0\n",
       "1   1\n",
       "2   2\n",
       "3   3\n",
       "4   4\n",
       "5   5\n",
       "6   6\n",
       "7   7\n",
       "8   8\n",
       "9   9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.from_pandas(pdf) # pandas on spark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "psdf = ps.range(10)\n",
    "sdf = psdf.to_spark().filter(\"id > 5\") ## ramki sprakowe i pandasowe są do siebie bardzo podobne, tu przekształcenie z ramki pandasowej na sparkową\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   6\n",
       "1   7\n",
       "2   8\n",
       "3   9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.to_pandas_on_spark() ## ze sparka do pandas on spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jak działa Pandas API?\n",
    "Gdy użytkownik tworzy ramkę danych Pandas-on-Spark, powstaje wtedy też „ramka wewnętrzna” i ramka sparkowa.\n",
    "\n",
    "„Ramka wewnętrzna” zapewnia konwersje między ramkami pandas-on-Spark i PySpark. Przechowuje metadane, takie jak mapowanie danych w kolumnach i indeksów.\n",
    "\n",
    "Pozwala ramce Pandas-on-Spark na obsługę funkcji Pandas, które nie są obsługiwane przez ramki PySpark:\n",
    "\n",
    "* mutable syntax: dzięki czemu nie trzeba tworzyć nowej ramki za każdym razem, gdy chcemy coś zmodyfikować\n",
    "* indeks sekwencyjny: żeby można było manipulować ramką w oparciu o indeks (patrz poniżej)\n",
    "* pandowe typy danych\n",
    "\n",
    "Należy pamiętać, że dane są rozproszone po wielu workerach, podczas gdy w Pandzie dane pozostają na jednej maszynie.\n",
    "\n",
    "### Indeksowanie\n",
    "\n",
    "W przeciwieństwie do ramki PySpark, ramki Pandas-on-Spark replikują funkcjonalność indeksowania Pandas (dzięki wspomianej wyżej wewnętrznej ramce). Dla przypomnienia, indeksy służą do uzyskiwania dostępu do wierszy przez indeksatory loc/iloc lub do mapowania właściwych wierszy w przypadku operacji łączących dwie ramki danych lub serie (na przykład df1 + df2) i tak dalej.\n",
    "\n",
    "Jeśli żadna z kolumn nie została określona jako indeks zostanie użyty indeks domyślny. Może to być jeden z 3:\n",
    "\n",
    "**Sekwencja** </br>\n",
    "Używany domyślnie. Implementuje sekwencję, która zwiększa się o jeden razem z każdym kolejnym rekordem. Najprawdopodobniej spowoduje to przeniesienie całej ramki do jednego klastra, co będzie bardzo powolne i najprawdopodoniej rzuci OOM error. Nie należy go używać, gdy zbiór danych jest duży."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2], dtype='int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.default_index_type', 'sequence')\n",
    "psdf = ps.range(3)\n",
    "ps.reset_option('compute.default_index_type')\n",
    "psdf.index\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rozproszona sekwencja** </br>\n",
    "Mechanizm jest ten sam, co wyżej, ale indeks rozproszony można wykorzystywać razem z partycjonowaniem. Powinien zostać użyty jeśli zbiór  danych jest duży i potrzebny jest indeks sekwencyjny. Należy zauważyć, że jeśli po utworzeniu tego indeksu do zbioru danych zostanie dodanych więcej rekordów nie ma gwarancji, że indeks pozostanie sekwencyjny.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2], dtype='int64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.default_index_type', 'distributed-sequence')\n",
    "psdf = ps.range(3)\n",
    "ps.reset_option('compute.default_index_type')\n",
    "psdf.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rozlokowany (distributed)** </br>\n",
    "Implementuje ciąg rosnący monotonicznie, ale nie nieprzerwany (np. 1, 8, 12), wartości są przypadkowe. Nie ma nic wspólnego z indeksowaniem Pandas. Pod względem wydajności jest najlepszy, ale nie można go używać do wykonywania operacji na dwóch ramkach. \n",
    "\n",
    "Poniżej przykład ustawiania typu indeksu, z któego chcemy skorzystać: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([42949672960, 85899345920, 128849018880], dtype='int64')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.default_index_type', 'distributed')\n",
    "psdf = ps.range(3)\n",
    "ps.reset_option('compute.default_index_type')\n",
    "psdf.index\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pozostałe opcje\n",
    "\n",
    "Pandas API ma system opcji, który pozwala dostosować niektóre aspekty jego pracy. Najczęściej użytkownicy zmieniają opcję wyświetlania wyników. Najważniejsze w tym wypadku są dwa polecenia: \n",
    "\n",
    "* get_option() / set_option() - podejrzenie/ustawienie opcji\n",
    "* reset_option() - zresteowanie danej opcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.get_option('compute.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.set_option('compute.max_rows', 2000)\n",
    "ps.get_option('compute.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.reset_option(\"display.max_rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.get_option('compute.max_rows')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operacje na różnych ramkach\n",
    "Pandas API domyślnie blokuje operacje na różnych ramkach (lub seriach), aby zapobiec zbyt ciężkim obliczeniowo operacjom. \n",
    "\n",
    "Można to włączyć, ustawiając compute.ops_on_diff_frames na True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id\n",
       "0 -5.0\n",
       "1 -3.0\n",
       "2 -1.0\n",
       "3  NaN\n",
       "4  NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.ops_on_diff_frames', True)\n",
    "psdf1 = ps.range(5)\n",
    "psdf2 = ps.DataFrame({'id': [5, 4, 3]})\n",
    "(psdf1 - psdf2).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.reset_option('compute.ops_on_diff_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>new_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  new_col\n",
       "0   0      1.0\n",
       "1   1      2.0\n",
       "2   2      3.0\n",
       "3   3      4.0\n",
       "4   4      NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "ps.set_option('compute.ops_on_diff_frames', True)\n",
    "psdf = ps.range(5)\n",
    "psser_a = ps.Series([1, 2, 3, 4])\n",
    "psdf['new_col'] = psser_a\n",
    "psdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wszystkie dostępne opcje można sprawdzić pod tym linkiem: https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#available-options"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform & apply\n",
    "\n",
    "Poniżej przedstawimy takie funkcje jak *DataFrame.transform()*, *DataFrame.apply()*, *DataFrame.pandas_on_spark.transform_batch()*, *DataFrame.pandas_on_spark.apply_batch()*, *Series.pandas_on_spark.transform_batch()*. Każda ma odrębny cel i działa inaczej. Poniżej przedstawimy te różnice, które najczęściej powoduję dezorientację. \n",
    "\n",
    "**transform & apply** </br>\n",
    "Główna różnica między *DataFrame.transform()* a *DataFrame.apply()* polega na tym, że ta pierwsza wymaga zwrócenia tej samej długości danych wejściowych, a druga tego nie. Przykład poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  2  5\n",
       "1  3  6\n",
       "2  4  7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf = ps.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "def pandas_plus(pser):\n",
    "   return pser + 1 \n",
    "\n",
    "psdf.transform(pandas_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  5\n",
       "2  3  7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf = ps.DataFrame({'a': [1,2,3], 'b':[5,6,7]})\n",
    "def pandas_plus(pser):\n",
    "     return pser[pser % 2 == 1]  # allows an arbitrary length\n",
    "\n",
    "psdf.apply(pandas_plus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każda funkcja przyjmuje serię pandas, a interfejs pandas przelicza funkcje w sposób rozproszony, jak poniżej: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku osi „kolumnowej” funkcja przyjmuje każdy wiersz jako serię:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    7\n",
       "2    9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf = ps.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "def pandas_plus(pser):\n",
    "     return sum(pser)  # allows an arbitrary length\n",
    "\n",
    "psdf.apply(pandas_plus, axis='columns')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### pandas_on_spark.transform_batch i pandas_on_spark.apply_batch\n",
    "W *DataFrame.pandas_on_spark.transform_batch()*, *DataFrame.pandas_on_spark.apply_batch()*, *Series.pandas_on_spark.transform_batch()* itp. przedrostek funkcji oznacza czy będziemy pracować z serią czy z ramką. Interfejsy dzielą Pandas-on-Spark DataFrame lub serię na partycje, a następnie stosują daną funkcję z ramką albo serią pandową w charakterze danych wejściowych i wyjściowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  2  5\n",
       "1  3  6\n",
       "2  4  7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf = ps.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "def pandas_plus(pdf):\n",
    "     return pdf + 1  # should always return the same length as input.\n",
    "\n",
    "psdf.pandas_on_spark.transform_batch(pandas_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "1  2  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf = ps.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "def pandas_plus(pdf):\n",
    "    return pdf[pdf.a > 1]  # allow arbitrary length\n",
    "\n",
    "psdf.pandas_on_spark.apply_batch(pandas_plus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcje w obu przykładach pobierają pandas DataFrame jako fragment pandas-on-Spark DataFrame i zwracają pandas DataFrame. \n",
    "Panuje ta sama zasada do poprzednio - transform() wymaga tej samej długości danych wyjściowych i wejściowych. Apply nie. \n",
    "\n",
    "W przypadku Series.pandas_on_spark.transform batch(), jest podobnie jak z DataFrame.pandas_on_spark.transform batch() - przyjmuje fragment serii pandas jako fragmentu serii pandas-on-Spark i zwraca serię pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    3\n",
       "2    4\n",
       "Name: a, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf = ps.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "def pandas_plus(pser):\n",
    "     return pser + 1  # should always return the same length as input.\n",
    "\n",
    "psdf.a.pandas_on_spark.transform_batch(pandas_plus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typy danych pandas on spark vs pyspark\n",
    "Podczas konwertowania Pandas-on-Spark DataFrame z/do PySpark DataFrame typy danych są automatycznie mapowane na odpowiedni typ. \n",
    "\n",
    "Poniższy przykład pokazuje, jak typy danych są mapowane z PySpark DataFrame na Pandas-on-Spark DataFrame. Podczas konwertowania pandas-on-Spark DataFrame na pandas DataFrame typy danych są w zasadzie takie same jak w pandas.\n",
    "\n",
    "Poniżej więcej informacji o mapowaniu między ramkami: https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/types.html#type-support-in-pandas-api-on-spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Przykłady wykorzystania funkcji pandas w Spark** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses    Fee Duration  Discount\n",
      "0    Spark  22000   30days    1000.0\n",
      "1  PySpark  25000   50days    2300.0\n",
      "2   Hadoop  23000   55days    1000.0\n",
      "3   Python  24000   40days    1200.0\n",
      "4   Pandas  26000   60days    2500.0\n",
      "5   Hadoop  25000   35days       NaN\n",
      "6    Spark  25000   30days    1400.0\n",
      "7   Python  22000   50days    1600.0\n",
      "8       NA   1500   40days       0.0\n",
      "           Fee  Discount\n",
      "Courses                 \n",
      "Spark    47000    2400.0\n",
      "PySpark  25000    2300.0\n",
      "Hadoop   48000    1000.0\n",
      "Python   46000    2800.0\n",
      "Pandas   26000    2500.0\n",
      "NA        1500       0.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark.pandas\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# Create pandas DataFrame\n",
    "technologies   = ({\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\",\"Hadoop\",\"Spark\",\"Python\",\"NA\"],\n",
    "    'Fee' :[22000,25000,23000,24000,26000,25000,25000,22000,1500],\n",
    "    'Duration':['30days','50days','55days','40days','60days','35days','30days','50days','40days'],\n",
    "    'Discount':[1000,2300,1000,1200,2500,None,1400,1600,0]\n",
    "          })\n",
    "df = ps.DataFrame(technologies)\n",
    "print(df)\n",
    "\n",
    "# Use groupby() to compute the sum\n",
    "df2 = df.groupby(['Courses']).sum()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.to_spark()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybieranie kolumn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Courses</th>\n",
       "      <th>Fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spark</td>\n",
       "      <td>22000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PySpark</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hadoop</td>\n",
       "      <td>23000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python</td>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pandas</td>\n",
       "      <td>26000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hadoop</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spark</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Python</td>\n",
       "      <td>22000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NA</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Courses    Fee\n",
       "0    Spark  22000\n",
       "1  PySpark  25000\n",
       "2   Hadoop  23000\n",
       "3   Python  24000\n",
       "4   Pandas  26000\n",
       "5   Hadoop  25000\n",
       "6    Spark  25000\n",
       "7   Python  22000\n",
       "8       NA   1500"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas API on Spark\n",
    "df[[\"Courses\",\"Fee\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Courses|  Fee|\n",
      "+-------+-----+\n",
      "|  Spark|22000|\n",
      "|PySpark|25000|\n",
      "| Hadoop|23000|\n",
      "| Python|24000|\n",
      "| Pandas|26000|\n",
      "| Hadoop|25000|\n",
      "|  Spark|25000|\n",
      "| Python|22000|\n",
      "|     NA| 1500|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark\n",
    "sdf.select(\"Courses\",\"Fee\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybieranie lub filtrowanie wierszy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Courses</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python</td>\n",
       "      <td>24000</td>\n",
       "      <td>40days</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Python</td>\n",
       "      <td>22000</td>\n",
       "      <td>50days</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Courses    Fee Duration  Discount\n",
       "3  Python  24000   40days    1200.0\n",
       "7  Python  22000   50days    1600.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas API on Spark\n",
    "df2 = df.loc[ (df.Courses == \"Python\")]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+--------+\n",
      "|Courses|  Fee|Duration|Discount|\n",
      "+-------+-----+--------+--------+\n",
      "| Python|24000|  40days|  1200.0|\n",
      "| Python|22000|  50days|  1600.0|\n",
      "+-------+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark\n",
    "sdf2 = sdf.filter(sdf.Courses == \"Python\")\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Courses     9\n",
       "Fee         9\n",
       "Duration    9\n",
       "Discount    8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas API on Spark\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PySpark\n",
    "sdf.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortowanie wierszy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas API on Spark\n",
    "df2 = df.sort_values([\"Courses\", \"Fee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+--------+\n",
      "|Courses|  Fee|Duration|Discount|\n",
      "+-------+-----+--------+--------+\n",
      "| Hadoop|23000|  55days|  1000.0|\n",
      "| Hadoop|25000|  35days|    null|\n",
      "|     NA| 1500|  40days|     0.0|\n",
      "| Pandas|26000|  60days|  2500.0|\n",
      "|PySpark|25000|  50days|  2300.0|\n",
      "| Python|22000|  50days|  1600.0|\n",
      "| Python|24000|  40days|  1200.0|\n",
      "|  Spark|22000|  30days|  1000.0|\n",
      "|  Spark|25000|  30days|  1400.0|\n",
      "+-------+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark\n",
    "sdf2 = sdf.sort(\"Courses\", \"Fee\")\n",
    "sdf2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmiana nazwy kolumny:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas API on Spark\n",
    "df2 = df.rename(columns={'Fee': 'Courses_Fee'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+--------+\n",
      "|Courses|Courses_Fee|Duration|Discount|\n",
      "+-------+-----------+--------+--------+\n",
      "|  Spark|      22000|  30days|  1000.0|\n",
      "|PySpark|      25000|  50days|  2300.0|\n",
      "| Hadoop|      23000|  55days|  1000.0|\n",
      "| Python|      24000|  40days|  1200.0|\n",
      "| Pandas|      26000|  60days|  2500.0|\n",
      "| Hadoop|      25000|  35days|    null|\n",
      "|  Spark|      25000|  30days|  1400.0|\n",
      "| Python|      22000|  50days|  1600.0|\n",
      "|     NA|       1500|  40days|     0.0|\n",
      "+-------+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark\n",
    "sdf2 = sdf.withColumnRenamed(\"Fee\", \"Courses_Fee\")\n",
    "sdf2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupBy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fee</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Courses</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spark</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PySpark</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hadoop</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Python</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pandas</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Fee  Duration  Discount\n",
       "Courses                         \n",
       "Spark      2         2         2\n",
       "PySpark    1         1         1\n",
       "Hadoop     2         2         1\n",
       "Python     2         2         2\n",
       "Pandas     1         1         1\n",
       "NA         1         1         1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas API on Spark\n",
    "df.groupby(['Courses']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Courses|count|\n",
      "+-------+-----+\n",
      "|  Spark|    2|\n",
      "|PySpark|    1|\n",
      "| Hadoop|    2|\n",
      "| Python|    2|\n",
      "| Pandas|    1|\n",
      "|     NA|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark\n",
    "sdf.groupBy(\"Courses\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista pandas API wspieranych przez PySpark:\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/supported_pandas_api.html#supported-pandas-api"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4ebc63f2cded5a5017438174b69eeaea6fe35f706fbd25d3a5f5d6de9a88d22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
