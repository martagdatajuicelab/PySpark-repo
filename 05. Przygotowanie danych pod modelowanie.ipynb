{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbf5237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-232E4IU.home:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataManipulation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x206ebc9e850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Może chwilę potrwać\n",
    "spark = SparkSession.builder.appName(\"DataManipulation\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efcc7b",
   "metadata": {},
   "source": [
    "# Sprawdzanie duplikatów, brakujących obserwacji, outlierów\n",
    "\n",
    "### Duplikaty\n",
    "\n",
    "Przyjrzyjmy się poniższej tabeli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9392d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(1, 144.5, 5.9, 33, 'M'),\n",
    "(2, 167.2, 5.4, 45, 'M'),\n",
    "(3, 124.1, 5.2, 23, 'F'),\n",
    "(4, 144.5, 5.9, 33, 'M'),\n",
    "(5, 133.2, 5.7, 54, 'F'),\n",
    "(3, 124.1, 5.2, 23, 'F'),\n",
    "(5, 129.2, 5.3, 42, 'M'),\n",
    "], ['id', 'weight', 'height', 'age', 'gender'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40318130",
   "metadata": {},
   "source": [
    "Jak widać mamy kilka problemów: \n",
    "* dwa wiersze o ID = 3, które mają te same wartości również w pozostałych kolumnach\n",
    "* wiersze o ID = 1 i ID = 4 są prawie takie same, różnią się tylko wartością ID. Dlatego możemy na spokojnie założyć, że to ta sama osoba\n",
    "\n",
    "Ten zbiór danych jest, oczywiście, bardzo mały, więc widać to na pierwszy rzut oka. Co zrobić jeśli mamy miliony rekordów? Warto zacząć od sprawdzenia ile jest duplikatów w zbiorze, porównując wynik funkcji *count()* na pełnym zbiorze z wynikiem funkcji *distinct()* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9057e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows: 7\n",
      "Count of distinct rows:6\n"
     ]
    }
   ],
   "source": [
    "print('Count of rows: {0}'.format(df.count()))\n",
    "print('Count of distinct rows:{0}'.format(df.distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62702c81",
   "metadata": {},
   "source": [
    "Jeżeli te dwa wyniki są różne - już wiadomo, że mamy duble. Można się ich pozbyć funkcją *.dropDuplicates()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446e5f1",
   "metadata": {},
   "source": [
    "Usunęliśmy w ten sposób duble wiersza o ID = 3. Pora na analizę pozostałych kolumn. Można to zrobić operując na okrojonej ramce - bez atrybutu ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5641e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of ids: 6\n",
      "Count of distinct ids: 5\n"
     ]
    }
   ],
   "source": [
    "print('Count of ids: {0}'.format(df.count()))\n",
    "print('Count of distinct ids: {0}'.format(\n",
    "df.select([\n",
    "c for c in df.columns if c != 'id'\n",
    "]).distinct().count())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac011a",
   "metadata": {},
   "source": [
    "Podobnie jak poprzednio, możemy używć funkcji *.dropDuplicates()*, ale wykluczając z operacji ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates(subset=[\n",
    "c for c in df.columns if c != 'id'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08630a6",
   "metadata": {},
   "source": [
    "Paramter *subset* sprawia, że *.dropDuplicates()* szuka duplikatów używając jedynie kolumn wymienionych w parametrze. W przykładzie powyżej pozbywamy się wierszy o tych samych wartościach wagi, wzrostu, wieku i płci, ale nie id. Spórzjmy na poniższy wynik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b39e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b527a",
   "metadata": {},
   "source": [
    "Uzyskaliśmy oczyszczony zbiór danych - nie ma duplikatów wierszu o ID = 3, a także nie ma wiersza 1/4. \n",
    "\n",
    "Skoro już wiemy, że nie zdublowanych (całych) wierszy ani nie ma wierszy łudząco do siebie podobnych można sprawdzić liczbę unikalnych IDków. Zrobimy to wykorzystując funkcję .agg(...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|count|distinct|\n",
      "+-----+--------+\n",
      "|    5|       4|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "df.agg(\n",
    "fn.count('id').alias('count'),\n",
    "fn.countDistinct('id').alias('distinct')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d6af9",
   "metadata": {},
   "source": [
    "Powyższy przykład rozpoczęliśmy od zaimportowania wszystkich potrzebnych funkcji z *pyspark.sql module*\n",
    "\n",
    "Następnie użyliśmy funkcji *.count()* i *.countDistinct()* żeby sprawdzić liczbę wierszy i liczbę unikalnych IDs. Funckja *.alias()* umożliwiła nadanie wynikowi innej nazwy.\n",
    "\n",
    "Jak widać powyżej mamy 5 wierszy, ale tylko 4 unikalne ID. W poprzednich krokach pozbyliśmy się duplikatów, więc możemy założyć, że nastąpiło jakieś zamieszanie w danych i żeby to wyprostować nadamy wierszom nowe ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81013b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+------+\n",
      "| id|weight|height|age|gender|new_id|\n",
      "+---+------+------+---+------+------+\n",
      "|  5| 133.2|   5.7| 54|     F|     0|\n",
      "|  1| 144.5|   5.9| 33|     M|     1|\n",
      "|  2| 167.2|   5.4| 45|     M|     2|\n",
      "|  3| 124.1|   5.2| 23|     F|     3|\n",
      "|  5| 129.2|   5.3| 42|     M|     4|\n",
      "+---+------+------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_id',\n",
    "fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5efac2",
   "metadata": {},
   "source": [
    "Funkcją *.monotonicallymonotonically_increasing_id()* nadaliśmy nowe, rosnące ID. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b72cf3",
   "metadata": {},
   "source": [
    "### Brakujące obserwacje\n",
    "\n",
    "Żeby nie zostać ze zbyt małą liczbą obserwacji w zbiorze danych warto sprawdzić, które atrybuty mają największe ubytki i rozważyć wykluczenie ich z analizy. Poza tym - jeśli zmienna ma prawie same braki jest bezużyteczna modelarsko. \n",
    "\n",
    "Inną metodą na poradzenie sobie z brakami jest imputacja. Można to zrobić na kilka sposobów: \n",
    "* dla danych Boolean - dodać kolejną kategorię np. \"missing\" \n",
    "* jeżeli dane już są kategoryczne - dodać jeszcze jeden poziom, \"missing\" właśnie\n",
    "* w przypadku danych porządkorwych albo numerycznych można imputować średnią, medianę, kwartylem, percentylem etc. \n",
    "\n",
    "Rozważmy przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa55c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  1| 143.5|   5.6|  28|     M|100000|\n",
      "|  2| 167.2|   5.4|  45|     M|  null|\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|  null|\n",
      "|  5| 133.2|   5.7|  54|     F|  null|\n",
      "|  6| 124.1|   5.2|null|     F|  null|\n",
      "|  7| 129.2|   5.3|  42|     M| 76000|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss = spark.createDataFrame([ (1, 143.5, 5.6, 28,\n",
    "'M', 100000),\n",
    "(2, 167.2, 5.4, 45, 'M', None),\n",
    "(3, None , 5.2, None, None, None),\n",
    "(4, 144.5, 5.9, 33, 'M', None),\n",
    "(5, 133.2, 5.7, 54, 'F', None),\n",
    "(6, 124.1, 5.2, None, 'F', None),\n",
    "(7, 129.2, 5.3, 42, 'M', 76000),\n",
    "], ['id', 'weight', 'height', 'age', 'gender', 'income'])\n",
    "\n",
    "df_miss.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18fa5c7",
   "metadata": {},
   "source": [
    "W przykładzie powyżej mamy do czynienia z kilkoma kategoriami missingów. Analizując wiersze widać, że: \n",
    "\n",
    "* wiersz z ID = 3 ma tylko jedną użyteczną informację - wzrost\n",
    "* kolumna \"income\" (dochód), w związku z tym, że to delikatna kwestia, ma bardzo dużo braków\n",
    "* wzrost i płeć mają tylko po jednym braku\n",
    "* wzrost ma dwa missingi\n",
    "\n",
    "Żeby policzyć missingi per wiersz można skorzystać z kodu poniżej: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6529ede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_miss.rdd.map(\n",
    "lambda row: (row['id'], sum([c == None for c in row]))\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83649b",
   "metadata": {},
   "source": [
    "Wynik należy czytać tak: wiersz o ID = 3 ma 4 brakujące wartości. Sprawdźmy co to za wartości żeby móc zdecydować czy je droppujemy czy coś imputujemy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68026ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.where('id == 3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9d1d4",
   "metadata": {},
   "source": [
    "Sprawdźmy teraz udział % missingów w każdej kolumnie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de22140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.agg(*[\n",
    "(1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')\n",
    "for c in df_miss.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d56cf",
   "metadata": {},
   "source": [
    "Mamy 14% missingów we wzroście i aż 72% missingów w przypadku dochodu. To dość jasne co należy z tym zrobić. Najpierw zdroppujemy dochód. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb865e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss_no_income = df_miss.select([\n",
    "c for c in df_miss.columns if c != 'income'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2818735",
   "metadata": {},
   "source": [
    "Dzięki temu już wiadomo, że nie trzeba usuwać wiersza ID = 3, bo kolumny \"wzrost\" i \"wiek\" mają wystarczająco dużo obserwacji żeby wyliczyć średnią i zastąpić nią braki. Gdybyśmy zdecydowali się je usunąć powinniśmy skorzystać z fukcji *.dropna(...)*, tak jak w przykładzie poniżej. W nim użyliśmy też parametru *tresh*, który pomaga wyspecyfikować jaki jest nasz poziom tolerancji na braków per wiersz. Parametr *tresh* przydaje się zwłaszcza wtedy kiedy mamy miliony rekordów i setki zmiennych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss_no_income.dropna(thresh=3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16e4d7",
   "metadata": {},
   "source": [
    "Z drugiej strony, gdyby zależało nam na imputacji brakujących obserwacji, moglibyśmy użyć funkcji *.fillna()*. Funkcja przyjmuje integery, floaty albo stringi. Wszystkie braki zostaną wypełnione przekazanym parametrem. Można jej także przekazać słownik: {'<kolumna>':'<wartość do zaimputowania>'}. Na słowniki nałożone są te same ograniczenia, co na *.fillna()*. \n",
    "Jeżeli chcemy zaimputować braki średnią czy medianą, musimy tę wartość najpierw policzyć albo stworzyć słownik wartości i potem przekazać je do *.fillna()*\n",
    "\n",
    "Poniżej przykład jak to zrobić:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb4607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------+---+-------+\n",
      "| id|            weight|height|age| gender|\n",
      "+---+------------------+------+---+-------+\n",
      "|  1|             143.5|   5.6| 28|      M|\n",
      "|  2|             167.2|   5.4| 45|      M|\n",
      "|  3|140.28333333333333|   5.2| 40|missing|\n",
      "|  4|             144.5|   5.9| 33|      M|\n",
      "|  5|             133.2|   5.7| 54|      F|\n",
      "|  6|             124.1|   5.2| 40|      F|\n",
      "|  7|             129.2|   5.3| 42|      M|\n",
      "+---+------------------+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "means = df_miss_no_income.agg(\n",
    "*[fn.mean(c).alias(c)\n",
    "for c in df_miss_no_income.columns if c != 'gender']\n",
    ").toPandas().to_dict('records')[0]\n",
    "means['gender'] = 'missing'\n",
    "df_miss_no_income.fillna(means).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0e89f",
   "metadata": {},
   "source": [
    "Pominęliśmy kolumnę \"płeć\" z wiadomych powodów. \n",
    "W przykładzie powyżej zastosowaliśmy podwójną konswersję - najpierw wynik .agg(...) (ramka PySparkowa) przekonwertowaliśmy  na ramkę pandasową, a potem na słownik. \n",
    "\n",
    "Parametr .to_dict(), pochodzący z pandas, instruuje sparka żeby stworzył powyższy słownik."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac2e57",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Outliery to obserwacje, które nie mieszą się w przedziale <Q1−1.5IQR; Q3+1.5IQR>. Spójrzmy na poniższy przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25b1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_outliers = spark.createDataFrame([\n",
    "(1, 143.5, 5.3, 28),\n",
    "(2, 154.2, 5.5, 45),\n",
    "(3, 342.3, 5.1, 99),\n",
    "(4, 144.5, 5.5, 33),\n",
    "(5, 133.2, 5.4, 54),\n",
    "(6, 124.1, 5.1, 21),\n",
    "(7, 129.2, 5.3, 42),\n",
    "], ['id', 'weight', 'height', 'age'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1294234",
   "metadata": {},
   "source": [
    "Teraz możemy wykorzystać rozstęp żeby oflagować outliery. Użyjemy do tego *.approxQuantile(...)*. Pierwszy argument, który przekażemy to nazwa kolumny, kolejnym może być albo numer między 0 a 1 (gdzie 0.5 oznacza policz medianę) albo lista (czyli to, co my zrobimy). Trzeci parametr oznacza poziom błędu, na który jesteśmy w stanie się zgodzić. Ustawienie go sprawi, że otrzymamy bardzo dokładny wynik, ale może być bardzo ciężki obliczeniowo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d22eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['weight', 'height', 'age'] ### nie mogę iteracyjnie dodać kolejnej wartości w słowniku\n",
    "bounds = {}\n",
    "for col in cols:\n",
    "    quantiles = df_outliers.approxQuantile(\n",
    "    col, [0.25, 0.75], 0.05\n",
    ")\n",
    "IQR = quantiles[1] - quantiles[0]\n",
    "curr_bound = {col:(quantiles[0] - 1.5 * IQR,\n",
    "quantiles[1] + 1.5 * IQR)}\n",
    "bounds.update(curr_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a9eb1",
   "metadata": {},
   "source": [
    "The bounds dictionary holds the lower and upper bounds for each\n",
    "feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fec78b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': (-11.0, 93.0)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cfe2d9",
   "metadata": {},
   "source": [
    "Użyjmy go do oznaczenia outlierów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "681d6fe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Marta Głowińska\\Desktop\\DJL PySpark\\warsztaty\\4. Przygotowanie danych pod modelowanie.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m outliers \u001b[39m=\u001b[39m df_outliers\u001b[39m.\u001b[39mselect(\u001b[39m*\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m (\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m (df_outliers[c] \u001b[39m<\u001b[39m bounds[c][\u001b[39m0\u001b[39m]) \u001b[39m|\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m (df_outliers[c] \u001b[39m>\u001b[39m bounds[c][\u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\u001b[39m.\u001b[39malias(c \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_o\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m cols])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m outliers\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;32mc:\\Users\\Marta Głowińska\\Desktop\\DJL PySpark\\warsztaty\\4. Przygotowanie danych pod modelowanie.ipynb Cell 41\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m outliers \u001b[39m=\u001b[39m df_outliers\u001b[39m.\u001b[39mselect(\u001b[39m*\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m (\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m (df_outliers[c] \u001b[39m<\u001b[39m bounds[c][\u001b[39m0\u001b[39m]) \u001b[39m|\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m (df_outliers[c] \u001b[39m>\u001b[39m bounds[c][\u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\u001b[39m.\u001b[39malias(c \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_o\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m cols])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta%20G%C5%82owi%C5%84ska/Desktop/DJL%20PySpark/warsztaty/4.%20Przygotowanie%20danych%20pod%20modelowanie.ipynb#X60sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m outliers\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'weight'"
     ]
    }
   ],
   "source": [
    "outliers = df_outliers.select(*['id'] + [\n",
    "(\n",
    "(df_outliers[c] < bounds[c][0]) |\n",
    "(df_outliers[c] > bounds[c][1])\n",
    ").alias(c + '_o') for c in cols])\n",
    "outliers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3e234",
   "metadata": {},
   "source": [
    "Mamy dwa outliery w wadze i dwa w wieku. Poniżej kod, który zwróci wartości istotnie różne od pozostałych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = df_outliers.join(outliers, on='id')\n",
    "df_outliers.filter('weight_o').select('id', 'weight').show()\n",
    "df_outliers.filter('age_o').select('id', 'age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c341237",
   "metadata": {},
   "source": [
    "Powyższy kod zwróci poniższą ramkę:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653866e",
   "metadata": {},
   "source": [
    "Mając informacje przytoczone w tej sekcji jesteśmy w szybko wyczyścić nawet bardzo duży zbiór danych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d44e3f",
   "metadata": {},
   "source": [
    "# Wstępne rozeznanie co siedzi w danych \n",
    "\n",
    "## Descriptive statistics\n",
    "Warto zacząć od statystyki opisowej, którą sprawdziwmy podstawowe informacje o zbiorze: średnie, odchylenia standardowe, wartości min i max itd. \n",
    "Zaczniemy od załadowania danych i przekonwertowania ich na ramkę Sparkową:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8836beec",
   "metadata": {},
   "source": [
    "First, we load the only module we will need. The pyspark.sql.types\n",
    "exposes all the data types we can use, such as IntegerType() or\n",
    "FloatType().\n",
    "\n",
    "Next, we read the data in and remove the header line using the\n",
    ".filter(...) method. This is followed by splitting the row on each\n",
    "comma (since this is a .csv file) and converting each element to an\n",
    "integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6f12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = spark.sparkContext.textFile('ccFraud.csv.gz')\n",
    "header = fraud.first()\n",
    "fraud = fraud \\\n",
    ".filter(lambda row: row != header) \\\n",
    ".map(lambda row: [int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc4ac4",
   "metadata": {},
   "source": [
    "Next, we create the schema for our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "*[\n",
    "typ.StructField(h[1:-1], typ.IntegerType(), True)\n",
    "for h in header.split(',')\n",
    "]\n",
    "] schema = typ.StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693ec39",
   "metadata": {},
   "source": [
    "Finally, we create our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = spark.createDataFrame(fraud, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27bd1d",
   "metadata": {},
   "source": [
    "Having created our fraud_df DataFrame, we can calculate the basic \n",
    "descriptive statistics for our dataset. However, you need to remember\n",
    "that even though all of our features appear as numeric in nature, some of\n",
    "them are categorical (for example, gender or state).\n",
    "Here's the schema of our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5968354",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67bebf",
   "metadata": {},
   "source": [
    "Also, no information would be gained from calculating the mean and\n",
    "standard deviation of the custId column, so we will not be doing that.\n",
    "For a better understanding of categorical columns, we will count the\n",
    "frequencies of their values using the .groupby(...) method. In this\n",
    "example, we will count the frequencies of the gender column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a525093",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df.groupby('gender').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c15825",
   "metadata": {},
   "source": [
    "The preceding code will produce the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0017ddf",
   "metadata": {},
   "source": [
    "As you can see, we are dealing with a fairly imbalanced dataset. What\n",
    "you would expect to see is an equal distribution for both genders.\n",
    "Note\n",
    "It goes beyond the scope of this chapter, but if we were building a\n",
    "statistical model, you would need to take care of these kinds of biases.\n",
    "For the truly numerical features, we can use the .describe() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb70a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['balance', 'numTrans', 'numIntlTrans']\n",
    "desc = fraud_df.describe(numerical)\n",
    "desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e14bfa",
   "metadata": {},
   "source": [
    "Even from these relatively few numbers we can tell quite a bit:\n",
    "All of the features are positively skewed. The maximum values are a\n",
    "number of times larger than the average.\n",
    "The coefficient of variation (the ratio of mean to standard deviation)\n",
    "is very high (close or greater than 1), suggesting a wide spread of\n",
    "observations.\n",
    "Here's how you check the skeweness (we will do it for the 'balance'\n",
    "feature only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df.agg({'balance': 'skewness'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d67a3",
   "metadata": {},
   "source": [
    "A list of aggregation functions (the names are fairly self-explanatory)\n",
    "includes: avg(), count(), countDistinct(), first(), kurtosis(),\n",
    "max(), mean(), min(), skewness(), stddev(), stddev_pop(),\n",
    "stddev_samp(), sum(), sumDistinct(), var_pop(), var_samp() and\n",
    "variance()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167eba2",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "Another highly useful measure of mutual relationships between features\n",
    "is correlation. Your model would normally include only those features\n",
    "that are highly correlated with your target. However, it is almost equally\n",
    "important to check the correlation between the features; including\n",
    "features that are highly correlated among them (that is, are collinear)\n",
    "may lead to unpredictable behavior of your model, or might unnecessarily complicate it.\n",
    "\n",
    "Calculating correlations in PySpark is very easy once your data is in a\n",
    "DataFrame form. The only difficulties are that the .corr(...) method\n",
    "supports the Pearson correlation coefficient at the moment, and it can\n",
    "only calculate pairwise correlations, such as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93782402",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df.corr('balance', 'numTrans')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9b100",
   "metadata": {},
   "source": [
    "In order to create a correlations matrix, you can use the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48098970",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_numerical = len(numerical)\n",
    "corr = []\n",
    "for i in range(0, n_numerical):\n",
    "temp = [None] * i\n",
    "for j in range(i, n_numerical):\n",
    "temp.append(fraud_df.corr(numerical[i], numerical[j]))\n",
    "corr.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17869b9",
   "metadata": {},
   "source": [
    "As you can see, the correlations between the numerical features in the\n",
    "credit card fraud dataset are pretty much non-existent. Thus, all these\n",
    "features can be used in our models, should they turn out to be statistically sound in explaining our target.\n",
    "Having checked the correlations, we can now move on to visually\n",
    "inspecting our data.\n",
    "\n",
    "# Visualization\n",
    "There are multiple visualization packages, but in this section we will be\n",
    "using matplotlib and Bokeh exclusively to give you the best tools for\n",
    "your needs.\n",
    "Both of the packages come preinstalled with Anaconda. First, let's load\n",
    "the modules and set them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f972d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import bokeh.charts as chrt\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc8faf1",
   "metadata": {},
   "source": [
    "The %matplotlib inline and the output_notebook() commands will\n",
    "make every chart generated with matplotlib or Bokeh, respectively,\n",
    "appear within the notebook and not as a separate window.\n",
    "Histograms\n",
    "Histograms are by far the easiest way to visually gauge the distribution\n",
    "of your features. There are three ways you can generate histograms in\n",
    "PySpark (or a Jupyter notebook):\n",
    "\n",
    "Aggregate the data in workers and return an aggregated list of bins\n",
    "and counts in each bin of the histogram to the driver\n",
    "Return all the data points to the driver and allow the plotting\n",
    "libraries' methods to do the job for you\n",
    "Sample your data and then return them to the driver for plotting.\n",
    "If the number of rows in your dataset is counted in billions, then the\n",
    "second option might not be attainable. Thus, you need to aggregate the\n",
    "data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ae1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = fraud_df.select('balance').rdd.flatMap(lambda row: row\n",
    ").histogram(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61994fca",
   "metadata": {},
   "source": [
    "To plot the histogram, you can simply call matplotlib, as shown in the\n",
    "following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "'bins': hists[0][:-1],\n",
    "'freq': hists[1]\n",
    "} plt.bar(data['bins'], data['freq'], width=2000)\n",
    "plt.title('Histogram of \\'balance\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697cf72",
   "metadata": {},
   "source": [
    "In a similar manner, a histogram can be created with Bokeh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eeb3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_hist = chrt.Bar(data,\n",
    "values='freq', label='bins',\n",
    "title='Histogram of \\'balance\\'')\n",
    "chrt.show(b_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d3459",
   "metadata": {},
   "source": [
    "Since Bokeh uses D3.js in the background, the resulting chart is\n",
    "interactive:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326808d",
   "metadata": {},
   "source": [
    "If your data is small enough to fit on the driver (although we would\n",
    "argue it would normally be faster to use the previous method), you can\n",
    "bring the data and use the .hist(...) (from matplotlib) or\n",
    ".Histogram(...) (from Bokeh) methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51472a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_driver = {\n",
    "'obs': fraud_df.select('balance').rdd.flatMap(\n",
    "lambda row: row\n",
    ").collect()\n",
    "}\n",
    "plt.hist(data_driver['obs'], bins=20)\n",
    "plt.title('Histogram of \\'balance\\' using .hist()')\n",
    "b_hist_driver = chrt.Histogram(\n",
    "data_driver, values='obs',\n",
    "title='Histogram of \\'balance\\' using .Histogram()',\n",
    "bins=20\n",
    ") chrt.show(b_hist_driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3e9f9",
   "metadata": {},
   "source": [
    "# Interactions between features\n",
    "Scatter charts allow us to visualize interactions between up to three\n",
    "variables at a time (although we will be only presenting a 2D interaction\n",
    "in this section).\n",
    "\n",
    "Since PySpark does not offer any visualization modules on the server\n",
    "side, and trying to plot billions of observations at the same time would be highly impractical, in this section we will sample the dataset at 0.02%\n",
    "(roughly 2,000 observations).\n",
    "\n",
    "In this example, we will sample our fraud dataset at 0.02% given\n",
    "'gender' as a strata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = fraud_df.sampleBy(\n",
    "'gender', {1: 0.0002, 2: 0.0002}\n",
    ").select(numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155bafa",
   "metadata": {},
   "source": [
    "To put multiple 2D charts in one go, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_multi = dict([\n",
    "(elem, data_sample.select(elem).rdd \\\n",
    ".flatMap(lambda row: row).collect())\n",
    "for elem in numerical\n",
    "])\n",
    "sctr = chrt.Scatter(data_multi, x='balance', y='numTrans')\n",
    "chrt.show(sctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0204b9",
   "metadata": {},
   "source": [
    "The preceding code will produce the following chart:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc09539",
   "metadata": {},
   "source": [
    "As you can see, there are plenty of fraudulent transactions that had 0\n",
    "balance but many transactions—that is, a fresh card and big spike of\n",
    "transactions. However, no specific pattern can be shown apart from\n",
    "some banding occurring at $1,000 intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f862b86c",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this chapter, we looked at how to clean and prepare your dataset for\n",
    "modeling by identifying and tackling datasets with missing values,\n",
    "duplicates, and outliers. We also looked at how to get a bit more familiar\n",
    "with your data using tools from PySpark (although this is by no means a\n",
    "full manual on how to analyze your datasets). Finally, we showed you\n",
    "how to chart your data.\n",
    "We will use these (and more) techniques in the next two chapters, where\n",
    "we will be building machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4ebc63f2cded5a5017438174b69eeaea6fe35f706fbd25d3a5f5d6de9a88d22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
